**Version:** 1.0.0 | **Last Updated:** 2024

Ù†Ø®Ø³ØªÛŒÙ† Ø¢Ø´Ù†Ø§ÛŒÛŒ
=====================================

.. contents:: Contents
    :depth: 3
    :local:

=====================================

Introduction | Ø¢Ø´Ù†Ø§ÛŒÛŒ
---------------------

.. tab-set::

    .. tab-item:: English

        Pasban is a pure Persian text processing library for detecting foreign (non-Persian) words. It offers both Aho-Corasick and regex-based detection engines, advanced normalization, and contextual cleaning. It is designed for high accuracy, speed, and extensibility.

        **Key Features:**

        * Fast multi-pattern matching with Aho-Corasick algorithm
        * Regex-based detection for maximum accuracy
        * Advanced Persian text normalization
        * Contextual cleaning and boundary detection
        * Comprehensive statistics and reporting
        * Extensible word database
        * Fully offline after initial setup

        **Links:**

        * GitHub: `pasban-py <https://github.com/keyaruga33/pasban-py>`_
        * Website: `pasban <https://example.com>`_

    .. tab-item:: Ù¾Ø§Ø±Ø³ÛŒ

        Ù¾Ø§Ø³Ø¨Ø§Ù† Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ† Ù¾Ø§Ø±Ø³ÛŒ Ø³Ø±Ù‡ Ø§Ø³Øª Ú©Ù‡ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ ÙˆØ§Ú˜Ú¯Ø§Ù† Ø¨ÛŒÚ¯Ø§Ù†Ù‡ Ø±Ø§ Ø¨Ø§ Ù…ÙˆØ´Ú©Ø§ÙÛŒ Ùˆ Ø³Ø±Ø¹Øª Ø¨Ø§Ù„Ø§ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ¯Ù‡Ø¯. Ø§ÛŒÙ† Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ Ù…ÙˆØªÙˆØ±Ù‡Ø§ÛŒ Ø¬Ø³ØªØ¬ÙˆÛŒ Ú†Ù†Ø¯Ø§Ù„Ú¯ÙˆÛŒÛŒ (Ø¢Ù‡Ùˆ-Ú©ÙØ±Ø§Ø³ÛŒÚ©) Ùˆ Ø¨Ø±Ù¾Ø§ÛŒÙ‡Ù” Ø¹Ø¨Ø§Ø±Øª Ù…Ù†Ø¸Ù…ØŒ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ùˆ Ù¾Ø§Ù„Ø§ÛŒØ´ Ø²Ù…ÛŒÙ†Ù‡â€ŒØ§ÛŒ Ø±Ø§ ÙØ±Ø§Ù‡Ù… Ù…ÛŒâ€ŒÚ©Ù†Ø¯.

        **ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ú©Ù„ÛŒØ¯ÛŒ:**

        * Ø¬Ø³ØªØ¬ÙˆÛŒ Ú†Ù†Ø¯Ø§Ù„Ú¯ÙˆÛŒÛŒ Ø³Ø±ÛŒØ¹ Ø¨Ø§ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ø¢Ù‡Ùˆ-Ú©ÙØ±Ø§Ø³ÛŒÚ©
        * Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø¨Ø±Ù¾Ø§ÛŒÙ‡Ù” Ø¹Ø¨Ø§Ø±Øª Ù…Ù†Ø¸Ù… Ø¨Ø±Ø§ÛŒ Ù…ÙˆØ´Ú©Ø§ÙÛŒ Ø¨ÛŒØ´ÛŒÙ†Ù‡
        * Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ù…ØªÙ† Ù¾Ø§Ø±Ø³ÛŒ
        * Ù¾Ø§Ù„Ø§ÛŒØ´ Ø²Ù…ÛŒÙ†Ù‡â€ŒØ§ÛŒ Ùˆ ØªØ´Ø®ÛŒØµ Ù…Ø±Ø²Ù‡Ø§ÛŒ ÙˆØ§Ú˜Ù‡
        * Ø¢Ù…Ø§Ø± Ùˆ Ú¯Ø²Ø§Ø±Ø´â€ŒØ¯Ù‡ÛŒ Ø¬Ø§Ù…Ø¹
        * Ù¾Ø§ÛŒÚ¯Ø§Ù‡ ÙˆØ§Ú˜Ú¯Ø§Ù† Ù‚Ø§Ø¨Ù„ Ú¯Ø³ØªØ±Ø´
        * Ú©Ø§Ù…Ù„Ø§Ù‹ Ø¢ÙÙ„Ø§ÛŒÙ† Ù¾Ø³ Ø§Ø² Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù†Ø®Ø³ØªÛŒÙ†

        **Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§:**

        * Ú¯ÛŒØªâ€ŒÙ‡Ø§Ø¨: `pasban-py <https://github.com/keyaruga33/pasban-py>`_
        * ØªØ§Ø±Ù†Ù…Ø§: `pasban <https://pasbans.ir>`_


Installation | Ù†ØµØ¨
------------------

.. tab-set::

    .. tab-item:: English

        .. code-block:: bash

            pip install pasban

        **Requirements:**

        * Python 3.8+
        * Internet connection (only for initial database download)

    .. tab-item:: Ù¾Ø§Ø±Ø³ÛŒ

        Ø¨Ø±Ø§ÛŒ Ù†ØµØ¨ Ù¾Ø§Ø³Ø¨Ø§Ù† Ú©Ø§ÙÛŒ Ø§Ø³Øª Ø¯Ø³ØªÙˆØ± Ø²ÛŒØ± Ø±Ø§ Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒØ¯:

        .. code-block:: bash

            pip install pasban

        **Ù¾ÛŒØ´â€ŒÙ†ÛŒØ§Ø²Ù‡Ø§:**

        * Ù¾Ø§ÛŒØªÙˆÙ† Û³.Û¸ ÛŒØ§ Ø¨Ø§Ù„Ø§ØªØ±
        * Ù¾ÛŒÙˆÙ†Ø¯ Ø¨Ù‡ Ø§ÛŒÙ†ØªØ±Ù†Øª (ØªÙ†Ù‡Ø§ Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ Ù†Ø®Ø³ØªÛŒÙ† Ù¾Ø§ÛŒÚ¯Ø§Ù‡â€ŒØ¯Ø§Ø¯Ù‡)

.. admonition:: Quick Start Example | Ù†Ù…ÙˆÙ†Ù‡ Ø´Ø±ÙˆØ¹ Ø³Ø±ÛŒØ¹
    :class: tip

    .. code-block:: python

        from pasban.detector import WordDetector

        # Initialize detector
        detector = WordDetector()

        # Detect foreign words
        text = "Ù…Ù† Ø¨Ø§ Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ± Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ù… Ùˆ Ø§Ø² Ø§ÛŒÙ†ØªØ±Ù†Øª Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ù…."
        result = detector.detect(text)

        # Print results
        print(f"Foreign words: {result.foreign_words}")
        print(f"Percentage: {result.foreign_percentage:.1f}%")
        print(f"\nReport:\n{result.to_summary_text}")


Modules | Ø³Ø§Ø®ØªØ§Ø± Ù…Ø§Ú˜ÙˆÙ„â€ŒÙ‡Ø§
-------------------------

.. tab-set::

    .. tab-item:: English

        * **detector**: Word detection engines (``WordDetector``, ``WordDetectorRegex``)
        * **db**: Word database access (``WordRepo``, ``DataLoader``)
        * **normalizer**: Text normalization and contextual cleaning
        * **core**: Core data types (``DetectData``, ``AhoCorasickAutomaton``)

    .. tab-item:: Ù¾Ø§Ø±Ø³ÛŒ

        * **detector**: Ù…ÙˆØªÙˆØ±Ù‡Ø§ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ ÙˆØ§Ú˜Ù‡ (``WordDetector`` Ùˆ ``WordDetectorRegex``)
        * **db**: Ù¾Ø§ÛŒÚ¯Ø§Ù‡ ÙˆØ§Ú˜Ú¯Ø§Ù† (``WordRepo`` Ùˆ ``DataLoader``)
        * **normalizer**: Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ùˆ Ù¾Ø§Ù„Ø§ÛŒØ´ Ù…ØªÙ†
        * **core**: Ø§Ù†ÙˆØ§Ø¹ Ø¯Ø§Ø¯Ù‡Ù” Ù¾Ø§ÛŒÙ‡ Ù…Ø§Ù†Ù†Ø¯ ``DetectData`` Ùˆ ``AhoCorasickAutomaton``



WordDetector | WordDetectorRegex
================================

.. tab-set::

    .. tab-item:: English

        **WordDetector** uses the Aho-Corasick automaton for fast, multi-pattern matching. It normalizes and contextually cleans text, ensuring accurate word boundaries. **WordDetectorRegex** uses a compiled regex pattern for sometimes higher accuracy.

        *Constructor*

        .. code-block:: python

            from pasban.detector import WordDetector, WordDetectorRegex

            detector = WordDetector()
            detector_regex = WordDetectorRegex()

        *Methods*

        .. py:method:: detect(text: str, normalize: bool = True, contextual: bool = True) -> DetectData

            Detect foreign words and return a DetectData object with full statistics.

            :param text: Input text to analyze
            :param normalize: Apply text normalization (default: True)
            :param contextual: Apply contextual cleaning (default: True)
            :return: DetectData object with detection results and statistics

        .. py:method:: detect_words(text: str, normalize: bool = True, contextual: bool = True) -> dict[str, str]

            Return only detected words and their Persian equivalents.

            :param text: Input text to analyze
            :param normalize: Apply text normalization (default: True)
            :param contextual: Apply contextual cleaning (default: True)
            :return: Dictionary mapping foreign words to Persian equivalents

        .. py:method:: find_words_in_text(text: str) -> list[str]

            Find all foreign words in text (WordDetectorRegex only).

            :param text: Input text to analyze
            :return: List of detected foreign words

        .. py:method:: reload() -> None

            Reload words from the database.

        *Example Usage*

        .. code-block:: python

            from pasban.detector import WordDetector

            # Initialize detector (one-time setup)
            detector = WordDetector()

            # Detect foreign words with full statistics
            text = "Ø§ÛŒÙ† Ù…ØªÙ† Ø´Ø§Ù…Ù„ Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ± Ùˆ Ø§ÛŒÙ†ØªØ±Ù†Øª Ø§Ø³Øª."
            result = detector.detect(text)

            # Access detection results
            print(f"Foreign words: {result.foreign_words}")
            # Output: ['Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±', 'Ø§ÛŒÙ†ØªØ±Ù†Øª']

            print(f"Mappings: {result.words}")
            # Output: {'Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±': 'Ø±Ø§ÛŒØ§Ù†Ù‡', 'Ø§ÛŒÙ†ØªØ±Ù†Øª': 'Ø§ÛŒÙ†ØªØ±Ù†Øª'}

            print(f"Total occurrences: {result.count}")
            # Output: 2

            print(f"Unique words: {result.unique_count}")
            # Output: 2

            print(f"Foreign percentage: {result.foreign_percentage:.2f}%")
            # Output: 28.57%

            # Get Persian report
            print(result.to_text)
            # Output: Ú¯Ø²Ø§Ø±Ø´ Ú©Ø§Ù…Ù„ Ø¨Ù‡ Ù¾Ø§Ø±Ø³ÛŒ

            # Get summary report
            print(result.to_summary_text)
            # Output: Ø®Ù„Ø§ØµÙ‡ Ø¢Ù…Ø§Ø±ÛŒ

            # Or just get the words dictionary
            words = detector.detect_words(text)
            print(words)
            # Output: {'Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±': 'Ø±Ø§ÛŒØ§Ù†Ù‡', 'Ø§ÛŒÙ†ØªØ±Ù†Øª': 'Ø§ÛŒÙ†ØªØ±Ù†Øª'}

        *Advanced Usage*

        .. code-block:: python

            from pasban.db import WordRepo

            repo = WordRepo()

            # Reload database from disk
            all_words = repo.get_all_words(reload=True)

            # Batch operations
            new_words = {
                "ÙˆØ¨Ø³Ø§ÛŒØª": "ÙˆØ¨Ú¯Ø§Ù‡",
                "Ø§ÛŒÙ…ÛŒÙ„": "Ø±Ø§ÛŒØ§Ù†Ø§Ù…Ù‡",
                "ÙØ§ÛŒÙ„": "Ù¾Ø±ÙˆÙ†Ø¯Ù‡"
            }

            for foreign, persian in new_words.items():
                repo.add_word(foreign, persian)

            # Search with different terms
            computer_words = repo.search_word("Ø±Ø§ÛŒØ§Ù†Ù‡", limit=10)
            print(f"Found {len(computer_words)} computer-related words")


    .. tab-item:: Ù¾Ø§Ø±Ø³ÛŒ

        **WordDetector** Ø¨Ø§ Ø¨Ù‡Ø±Ù‡â€ŒÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø¢Ù‡Ùˆ-Ú©ÙØ±Ø§Ø³ÛŒÚ©ØŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ ÙˆØ§Ú˜Ú¯Ø§Ù† Ø¨ÛŒÚ¯Ø§Ù†Ù‡ Ø±Ø§ Ø¨Ø§ Ø³Ø±Ø¹Øª Ùˆ Ù…ÙˆØ´Ú©Ø§ÙÛŒ Ø¨Ø§Ù„Ø§ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ¯Ù‡Ø¯. **WordDetectorRegex** Ø¨Ø§ Ø¹Ø¨Ø§Ø±Øª Ù…Ù†Ø¸Ù… Ú¯Ø§Ù‡ÛŒ Ù…ÙˆØ´Ú©Ø§ÙÛŒ Ø¨ÛŒØ´ØªØ±ÛŒ Ø¯Ø§Ø±Ø¯.

        *Ø³Ø§Ø²Ù†Ø¯Ù‡*

        .. code-block:: python

            from pasban.detector import WordDetector, WordDetectorRegex

            detector = WordDetector()
            detector_regex = WordDetectorRegex()

        *Ù…ØªØ¯Ù‡Ø§*

        .. py:method:: detect(text: str, normalize: bool = True, contextual: bool = True) -> DetectData
            :no-index:

            Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ ÙˆØ§Ú˜Ú¯Ø§Ù† Ø¨ÛŒÚ¯Ø§Ù†Ù‡ Ùˆ Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† Ø´ÛŒØ¡ DetectData Ø¨Ø§ Ø¢Ù…Ø§Ø± Ú©Ø§Ù…Ù„.

            :param text: Ù…ØªÙ† ÙˆØ±ÙˆØ¯ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„
            :param normalize: Ø§Ø¹Ù…Ø§Ù„ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù…ØªÙ† (Ù¾ÛŒØ´â€ŒÙØ±Ø¶: True)
            :param contextual: Ø§Ø¹Ù…Ø§Ù„ Ù¾Ø§Ù„Ø§ÛŒØ´ Ø²Ù…ÛŒÙ†Ù‡â€ŒØ§ÛŒ (Ù¾ÛŒØ´â€ŒÙØ±Ø¶: True)
            :return: Ø´ÛŒØ¡ DetectData Ø¨Ø§ Ù†ØªØ§ÛŒØ¬ Ùˆ Ø¢Ù…Ø§Ø± Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ

        .. py:method:: detect_words(text: str, normalize: bool = True, contextual: bool = True) -> dict[str, str]
            :no-index:

            Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† ÙÙ‚Ø· ÙˆØ§Ú˜Ú¯Ø§Ù† Ø¨ÛŒÚ¯Ø§Ù†Ù‡ Ùˆ Ø¨Ø±Ø§Ø¨Ø± Ù¾Ø§Ø±Ø³ÛŒ Ø¢Ù†â€ŒÙ‡Ø§.

            :param text: Ù…ØªÙ† ÙˆØ±ÙˆØ¯ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„
            :param normalize: Ø§Ø¹Ù…Ø§Ù„ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù…ØªÙ† (Ù¾ÛŒØ´â€ŒÙØ±Ø¶: True)
            :param contextual: Ø§Ø¹Ù…Ø§Ù„ Ù¾Ø§Ù„Ø§ÛŒØ´ Ø²Ù…ÛŒÙ†Ù‡â€ŒØ§ÛŒ (Ù¾ÛŒØ´â€ŒÙØ±Ø¶: True)
            :return: Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ù†Ú¯Ø§Ø´Øª ÙˆØ§Ú˜Ú¯Ø§Ù† Ø¨ÛŒÚ¯Ø§Ù†Ù‡ Ø¨Ù‡ Ø¨Ø±Ø§Ø¨Ø± Ù¾Ø§Ø±Ø³ÛŒ

        .. py:method:: find_words_in_text(text: str) -> list[str]
            :no-index:

            ÛŒØ§ÙØªÙ† Ù‡Ù…Ù‡Ù” ÙˆØ§Ú˜Ú¯Ø§Ù† Ø¨ÛŒÚ¯Ø§Ù†Ù‡ Ø¯Ø± Ù…ØªÙ† (ÙÙ‚Ø· WordDetectorRegex).

            :param text: Ù…ØªÙ† ÙˆØ±ÙˆØ¯ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„
            :return: ÙÙ‡Ø±Ø³Øª ÙˆØ§Ú˜Ú¯Ø§Ù† Ø¨ÛŒÚ¯Ø§Ù†Ù‡ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒâ€ŒØ´Ø¯Ù‡

        .. py:method:: reload() -> None
            :no-index:

            Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯ÙˆØ¨Ø§Ø±Ù‡ ÙˆØ§Ú˜Ú¯Ø§Ù† Ø§Ø² Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡.

        *Ù†Ù…ÙˆÙ†Ù‡ Ú©Ø§Ø±Ø¨Ø±Ø¯*

        .. code-block:: python

            from pasban.detector import WordDetector

            # Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø´Ù†Ø§Ø³Ø§Ú¯Ø± (Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ ÛŒÚ©â€ŒØ¨Ø§Ø±)
            detector = WordDetector()

            # Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ ÙˆØ§Ú˜Ú¯Ø§Ù† Ø¨ÛŒÚ¯Ø§Ù†Ù‡ Ø¨Ø§ Ø¢Ù…Ø§Ø± Ú©Ø§Ù…Ù„
            text = "Ø§ÛŒÙ† Ù…ØªÙ† Ø´Ø§Ù…Ù„ Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ± Ùˆ Ø§ÛŒÙ†ØªØ±Ù†Øª Ø§Ø³Øª."
            result = detector.detect(text)

            # Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ Ù†ØªØ§ÛŒØ¬ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ
            print(f"ÙˆØ§Ú˜Ú¯Ø§Ù† Ø¨ÛŒÚ¯Ø§Ù†Ù‡: {result.foreign_words}")
            # Ø®Ø±ÙˆØ¬ÛŒ: ['Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±', 'Ø§ÛŒÙ†ØªØ±Ù†Øª']

            print(f"Ù†Ú¯Ø§Ø´Øªâ€ŒÙ‡Ø§: {result.words}")
            # Ø®Ø±ÙˆØ¬ÛŒ: {'Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±': 'Ø±Ø§ÛŒØ§Ù†Ù‡', 'Ø§ÛŒÙ†ØªØ±Ù†Øª': 'Ø§ÛŒÙ†ØªØ±Ù†Øª'}

            print(f"ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ø±Ø®Ø¯Ø§Ø¯Ù‡Ø§: {result.count}")
            # Ø®Ø±ÙˆØ¬ÛŒ: 2

            print(f"ÙˆØ§Ú˜Ú¯Ø§Ù† ÛŒÚ©ØªØ§: {result.unique_count}")
            # Ø®Ø±ÙˆØ¬ÛŒ: 2

            print(f"Ø¯Ø±ØµØ¯ ÙˆØ§Ú˜Ú¯Ø§Ù† Ø¨ÛŒÚ¯Ø§Ù†Ù‡: {result.foreign_percentage:.2f}%")
            # Ø®Ø±ÙˆØ¬ÛŒ: 28.57%

            # Ø¯Ø±ÛŒØ§ÙØª Ú¯Ø²Ø§Ø±Ø´ Ù¾Ø§Ø±Ø³ÛŒ
            print(result.to_text)
            # Ø®Ø±ÙˆØ¬ÛŒ: Ú¯Ø²Ø§Ø±Ø´ Ú©Ø§Ù…Ù„ Ø¨Ù‡ Ù¾Ø§Ø±Ø³ÛŒ

            # Ø¯Ø±ÛŒØ§ÙØª Ø®Ù„Ø§ØµÙ‡ Ø¢Ù…Ø§Ø±ÛŒ
            print(result.to_summary_text)
            # Ø®Ø±ÙˆØ¬ÛŒ: Ø®Ù„Ø§ØµÙ‡ Ø¢Ù…Ø§Ø±ÛŒ

            # ÛŒØ§ ÙÙ‚Ø· Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ ÙˆØ§Ú˜Ú¯Ø§Ù† Ø±Ø§ Ø¯Ø±ÛŒØ§ÙØª Ú©Ù†ÛŒØ¯
            words = detector.detect_words(text)
            print(words)
            # Ø®Ø±ÙˆØ¬ÛŒ: {'Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±': 'Ø±Ø§ÛŒØ§Ù†Ù‡', 'Ø§ÛŒÙ†ØªØ±Ù†Øª': 'Ø§ÛŒÙ†ØªØ±Ù†Øª'}

        *Ú©Ø§Ø±Ø¨Ø±Ø¯ Ù¾ÛŒØ´Ø±ÙØªÙ‡*

        .. code-block:: python

            from pasban.detector import WordDetector, WordDetectorRegex

            # Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø¨Ø¯ÙˆÙ† Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ
            result = detector.detect(text, normalize=False)

            # Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø¨Ø¯ÙˆÙ† Ù¾Ø§Ù„Ø§ÛŒØ´ Ø²Ù…ÛŒÙ†Ù‡â€ŒØ§ÛŒ
            result = detector.detect(text, contextual=False)

            # Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø¨Ø§ ØºÛŒØ±ÙØ¹Ø§Ù„ Ú©Ø±Ø¯Ù† Ù‡Ø± Ø¯Ùˆ (Ø³Ø±ÛŒØ¹â€ŒØªØ±ÛŒÙ†ØŒ Ú©Ù…â€ŒÙ…ÙˆØ´Ú©Ø§ÙÛŒâ€ŒØªØ±ÛŒÙ†)
            result = detector.detect(text, normalize=False, contextual=False)

            # Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù‡Ø± Ø¯Ùˆ Ù…ÙˆØªÙˆØ±
            detector_ac = WordDetector()
            detector_re = WordDetectorRegex()

            result_ac = detector_ac.detect(text)
            result_re = detector_re.detect(text)

            print(f"Ø¢Ù‡Ùˆ-Ú©ÙØ±Ø§Ø³ÛŒÚ© ÛŒØ§ÙØª: {result_ac.count} ÙˆØ§Ú˜Ù‡")
            print(f"Ø¹Ø¨Ø§Ø±Øª Ù…Ù†Ø¸Ù… ÛŒØ§ÙØª: {result_re.count} ÙˆØ§Ú˜Ù‡")

            # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ù¾Ø§ÛŒÚ¯Ø§Ù‡â€ŒØ¯Ø§Ø¯Ù‡ Ù¾Ø³ Ø§Ø² Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ
            detector.reload()

Which Detector Should I Use? | Ú©Ø¯Ø§Ù… Ù…ÙˆØªÙˆØ± Ø±Ø§ Ø¨Ø±Ú¯Ø²ÛŒÙ†Ù…ØŸ
-----------------------------------------------------

.. tab-set::

    .. tab-item:: English

        Pasban provides two main detection engines:

        * **WordDetector (Aho-Corasick)**: Extremely fast for large texts and wordlists; slightly less accurate in rare edge cases; **recommended for most applications**.
        * **WordDetectorRegex**: More accurate (especially with complex boundaries) but slower; best for small datasets or when maximum precision is needed.

        .. grid:: 2
            :gutter: 3

            .. grid-item-card:: WordDetector
                :class-card: sd-border-success

                âœ… **Recommended for most use cases**

                * Extremely fast (10-20x faster)
                * Excellent for large-scale processing
                * Real-time applications
                * Batch processing
                * Memory efficient

            .. grid-item-card:: WordDetectorRegex
                :class-card: sd-border-info

                ğŸ¯ **For maximum accuracy**

                * Higher precision
                * Better boundary detection
                * Small text processing
                * Critical accuracy scenarios
                * Educational/research use

    .. tab-item:: Ù¾Ø§Ø±Ø³ÛŒ

        Ù¾Ø§Ø³Ø¨Ø§Ù† Ø¯Ùˆ Ù…ÙˆØªÙˆØ± Ø§ØµÙ„ÛŒ Ø¯Ø§Ø±Ø¯:

        * **WordDetector (Ø¢Ù‡Ùˆ-Ú©ÙØ±Ø§Ø³ÛŒÚ©)**: Ø¨Ø±Ø§ÛŒ Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ Ø¨Ø²Ø±Ú¯ Ø¨Ø³ÛŒØ§Ø± Ø³Ø±ÛŒØ¹Ø› Ø¯Ø± Ù…ÙˆØ§Ø±Ø¯ Ù†Ø§Ø¯Ø± Ú©Ù…ÛŒ Ú©Ù…â€ŒÙ…ÙˆØ´Ú©Ø§ÙÛŒâ€ŒØªØ±Ø› **Ø¨Ø±Ø§ÛŒ Ø§Ú©Ø«Ø± Ú©Ø§Ø±Ø¨Ø±Ø¯Ù‡Ø§ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ù…ÛŒâ€ŒØ´ÙˆØ¯**.
        * **WordDetectorRegex**: Ø¯Ù‚ÛŒÙ‚â€ŒØªØ± (Ø¨Ù‡â€ŒÙˆÛŒÚ˜Ù‡ Ø¯Ø± Ù…Ø±Ø²Ù‡Ø§ÛŒ Ù¾ÛŒÚ†ÛŒØ¯Ù‡) Ø§Ù…Ø§ Ú©Ù†Ø¯ØªØ±Ø› Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú©ÙˆÚ†Ú© ÛŒØ§ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ù…ÙˆØ´Ú©Ø§ÙÛŒ Ø¨ÛŒØ´ÛŒÙ†Ù‡ Ù…Ù†Ø§Ø³Ø¨ Ø§Ø³Øª.

        .. grid:: 2
            :gutter: 3

            .. grid-item-card:: WordDetector
                :class-card: sd-border-success

                âœ… **Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ø¨Ø±Ø§ÛŒ Ø§Ú©Ø«Ø± Ú©Ø§Ø±Ø¨Ø±Ø¯Ù‡Ø§**

                * Ø¨Ø³ÛŒØ§Ø± Ø³Ø±ÛŒØ¹ (Û±Û°-Û²Û° Ø¨Ø±Ø§Ø¨Ø± Ø³Ø±ÛŒØ¹â€ŒØªØ±)
                * Ø¹Ø§Ù„ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø± Ù…Ù‚ÛŒØ§Ø³ Ø¨Ø²Ø±Ú¯
                * Ú©Ø§Ø±Ø¨Ø±Ø¯Ù‡Ø§ÛŒ Ø¨Ù„Ø§Ø¯Ø±Ù†Ú¯
                * Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ
                * Ú©Ø§Ø±Ø¢Ù…Ø¯ Ø¯Ø± Ø­Ø§ÙØ¸Ù‡

            .. grid-item-card:: WordDetectorRegex
                :class-card: sd-border-info

                ğŸ¯ **Ø¨Ø±Ø§ÛŒ Ù…ÙˆØ´Ú©Ø§ÙÛŒ Ø¨ÛŒØ´ÛŒÙ†Ù‡**

                * Ù…ÙˆØ´Ú©Ø§ÙÛŒ Ø¨Ø§Ù„Ø§ØªØ±
                * ØªØ´Ø®ÛŒØµ Ø¨Ù‡ØªØ± Ù…Ø±Ø²Ù‡Ø§
                * Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ Ú©ÙˆÚ†Ú©
                * Ø³Ù†Ø§Ø±ÛŒÙˆÙ‡Ø§ÛŒ Ø­Ø³Ø§Ø³ Ø¨Ù‡ Ù…ÙˆØ´Ú©Ø§ÙÛŒ
                * Ú©Ø§Ø±Ø¨Ø±Ø¯ Ø¢Ù…ÙˆØ²Ø´ÛŒ/Ù¾Ú˜ÙˆÙ‡Ø´ÛŒ


Performance Benchmark | Ø³Ù†Ø¬Ø´ Ú©Ø§Ø±Ø§ÛŒÛŒ
------------------------------------

.. tab-set::

    .. tab-item:: English

        Comprehensive benchmark results on Intel Core i7-8565U (100 iterations):

        .. tab-set::

            .. tab-item:: BigText (1216 chars, 46 foreign words)

                .. list-table::
                    :header-rows: 1
                    :widths: 20 20 20 20 20

                    * - Engine
                      - Operation
                      - Average
                      - Min/Max
                      - StdDev
                    * - **WordDetector**
                      - Init
                      - 0.086s
                      - 0.081s / 0.099s
                      - 0.003s
                    * -
                      - **Detect**
                      - **0.000650s**
                      - 0.000562s / 0.000975s
                      - 0.000084s
                    * - **WordDetectorRegex**
                      - Init
                      - 0.039s
                      - 0.035s / 0.186s
                      - 0.021s
                    * -
                      - **Detect**
                      - **0.012093s**
                      - 0.011914s / 0.013110s
                      - 0.000191s

                .. important::

                    WordDetector is **~18.6x faster** on large texts!

            .. tab-item:: SmallText (86 chars, 3 foreign words)

                .. list-table::
                    :header-rows: 1
                    :widths: 20 20 20 20 20

                    * - Engine
                      - Operation
                      - Average
                      - Min/Max
                      - StdDev
                    * - **WordDetector**
                      - Init
                      - 0.084s
                      - 0.079s / 0.090s
                      - 0.002s
                    * -
                      - **Detect**
                      - **0.000054s**
                      - 0.000050s / 0.000100s
                      - 0.000007s
                    * - **WordDetectorRegex**
                      - Init
                      - 0.036s
                      - 0.034s / 0.040s
                      - 0.001s
                    * -
                      - **Detect**
                      - **0.000917s**
                      - 0.000888s / 0.001149s
                      - 0.000040s

                .. important::

                    WordDetector is **~17x faster** on small texts!

            .. tab-item:: PurePersian (94 chars, 0 foreign words)

                .. list-table::
                    :header-rows: 1
                    :widths: 20 20 20 20 20

                    * - Engine
                      - Operation
                      - Average
                      - Min/Max
                      - StdDev
                    * - **WordDetector**
                      - Init
                      - 0.089s
                      - 0.081s / 0.122s
                      - 0.009s
                    * -
                      - **Detect**
                      - **0.000038s**
                      - 0.000037s / 0.000070s
                      - 0.000005s
                    * - **WordDetectorRegex**
                      - Init
                      - 0.037s
                      - 0.036s / 0.046s
                      - 0.002s
                    * -
                      - **Detect**
                      - **0.001151s**
                      - 0.001115s / 0.001396s
                      - 0.000050s

                .. important::

                    WordDetector is **~30x faster** on pure Persian text!

        .. admonition:: Performance Summary
            :class: tip

            * **WordDetector** initialization: ~85ms (one-time cost)
            * **WordDetector** detection: 0.04-0.65ms per text
            * **WordDetectorRegex** initialization: ~37ms (one-time cost)
            * **WordDetectorRegex** detection: 0.9-12ms per text
            * **Speed advantage**: WordDetector is 17-30x faster for detection
            * **Accuracy difference**: < 2% in most cases

    .. tab-item:: Ù¾Ø§Ø±Ø³ÛŒ

        Ù†ØªØ§ÛŒØ¬ Ø³Ù†Ø¬Ø´ Ø¬Ø§Ù…Ø¹ Ø¨Ø± Ø±ÙˆÛŒ Intel Core i7-8565U (Û±Û°Û° ØªÚ©Ø±Ø§Ø±):

        .. tab-set::

            .. tab-item:: Ù…ØªÙ† Ø¨Ø²Ø±Ú¯ (Û±Û²Û±Û¶ Ù†ÙˆÛŒØ³Ù‡ØŒ Û´Û¶ ÙˆØ§Ú˜Ù‡ Ø¨ÛŒÚ¯Ø§Ù†Ù‡)

                .. list-table::
                    :header-rows: 1
                    :widths: 20 20 20 20 20

                    * - Ù…ÙˆØªÙˆØ±
                      - Ø¹Ù…Ù„ÛŒØ§Øª
                      - Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†
                      - Ú©Ù…ÛŒÙ†Ù‡/Ø¨ÛŒØ´ÛŒÙ†Ù‡
                      - Ø§Ù†Ø­Ø±Ø§Ù Ù…Ø¹ÛŒØ§Ø±
                    * - **WordDetector**
                      - Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ
                      - 0.086s
                      - 0.081s / 0.099s
                      - 0.003s
                    * -
                      - **Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ**
                      - **0.000650s**
                      - 0.000562s / 0.000975s
                      - 0.000084s
                    * - **WordDetectorRegex**
                      - Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ
                      - 0.039s
                      - 0.035s / 0.186s
                      - 0.021s
                    * -
                      - **Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ**
                      - **0.012093s**
                      - 0.011914s / 0.013110s
                      - 0.000191s

                .. important::

                    WordDetector Ø¯Ø± Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ Ø¨Ø²Ø±Ú¯ **~Û±Û¸.Û¶ Ø¨Ø±Ø§Ø¨Ø± Ø³Ø±ÛŒØ¹â€ŒØªØ±** Ø§Ø³Øª!

            .. tab-item:: Ù…ØªÙ† Ú©ÙˆÚ†Ú© (Û¸Û¶ Ù†ÙˆÛŒØ³Ù‡ØŒ Û³ ÙˆØ§Ú˜Ù‡ Ø¨ÛŒÚ¯Ø§Ù†Ù‡)

                .. list-table::
                    :header-rows: 1
                    :widths: 20 20 20 20 20

                    * - Ù…ÙˆØªÙˆØ±
                      - Ø¹Ù…Ù„ÛŒØ§Øª
                      - Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†
                      - Ú©Ù…ÛŒÙ†Ù‡/Ø¨ÛŒØ´ÛŒÙ†Ù‡
                      - Ø§Ù†Ø­Ø±Ø§Ù Ù…Ø¹ÛŒØ§Ø±
                    * - **WordDetector**
                      - Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ
                      - 0.084s
                      - 0.079s / 0.090s
                      - 0.002s
                    * -
                      - **Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ**
                      - **0.000054s**
                      - 0.000050s / 0.000100s
                      - 0.000007s
                    * - **WordDetectorRegex**
                      - Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ
                      - 0.036s
                      - 0.034s / 0.040s
                      - 0.001s
                    * -
                      - **Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ**
                      - **0.000917s**
                      - 0.000888s / 0.001149s
                      - 0.000040s

                .. important::

                    WordDetector Ø¯Ø± Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ Ú©ÙˆÚ†Ú© **~Û±Û· Ø¨Ø±Ø§Ø¨Ø± Ø³Ø±ÛŒØ¹â€ŒØªØ±** Ø§Ø³Øª!

            .. tab-item:: Ù…ØªÙ† Ù¾Ø§Ø±Ø³ÛŒ Ø³Ø±Ù‡ (Û¹Û´ Ù†ÙˆÛŒØ³Ù‡ØŒ Û° ÙˆØ§Ú˜Ù‡ Ø¨ÛŒÚ¯Ø§Ù†Ù‡)

                .. list-table::
                    :header-rows: 1
                    :widths: 20 20 20 20 20

                    * - Ù…ÙˆØªÙˆØ±
                      - Ø¹Ù…Ù„ÛŒØ§Øª
                      - Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†
                      - Ú©Ù…ÛŒÙ†Ù‡/Ø¨ÛŒØ´ÛŒÙ†Ù‡
                      - Ø§Ù†Ø­Ø±Ø§Ù Ù…Ø¹ÛŒØ§Ø±
                    * - **WordDetector**
                      - Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ
                      - 0.089s
                      - 0.081s / 0.122s
                      - 0.009s
                    * -
                      - **Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ**
                      - **0.000038s**
                      - 0.000037s / 0.000070s
                      - 0.000005s
                    * - **WordDetectorRegex**
                      - Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ
                      - 0.037s
                      - 0.036s / 0.046s
                      - 0.002s
                    * -
                      - **Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ**
                      - **0.001151s**
                      - 0.001115s / 0.001396s
                      - 0.000050s

                .. important::

                    WordDetector Ø¯Ø± Ù…ØªÙ† Ù¾Ø§Ø±Ø³ÛŒ Ø³Ø±Ù‡ **~Û³Û° Ø¨Ø±Ø§Ø¨Ø± Ø³Ø±ÛŒØ¹â€ŒØªØ±** Ø§Ø³Øª!

        .. admonition:: Ø®Ù„Ø§ØµÙ‡ Ú©Ø§Ø±Ø§ÛŒÛŒ
            :class: tip

            * **WordDetector** Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ: ~Û¸Ûµ Ù…ÛŒÙ„ÛŒâ€ŒØ«Ø§Ù†ÛŒÙ‡ (Ù‡Ø²ÛŒÙ†Ù‡ ÛŒÚ©â€ŒØ¨Ø§Ø±)
            * **WordDetector** Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ: Û°.Û°Û´-Û°.Û¶Ûµ Ù…ÛŒÙ„ÛŒâ€ŒØ«Ø§Ù†ÛŒÙ‡ Ø¨Ù‡ Ø§Ø²Ø§ÛŒ Ù‡Ø± Ù…ØªÙ†
            * **WordDetectorRegex** Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ: ~Û³Û· Ù…ÛŒÙ„ÛŒâ€ŒØ«Ø§Ù†ÛŒÙ‡ (Ù‡Ø²ÛŒÙ†Ù‡ ÛŒÚ©â€ŒØ¨Ø§Ø±)
            * **WordDetectorRegex** Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ: Û°.Û¹-Û±Û² Ù…ÛŒÙ„ÛŒâ€ŒØ«Ø§Ù†ÛŒÙ‡ Ø¨Ù‡ Ø§Ø²Ø§ÛŒ Ù‡Ø± Ù…ØªÙ†
            * **Ø¨Ø±ØªØ±ÛŒ Ø³Ø±Ø¹Øª**: WordDetector Ø¯Ø± Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Û±Û·-Û³Û° Ø¨Ø±Ø§Ø¨Ø± Ø³Ø±ÛŒØ¹â€ŒØªØ± Ø§Ø³Øª
            * **ØªÙØ§ÙˆØª Ù…ÙˆØ´Ú©Ø§ÙÛŒ**: Ø¯Ø± Ø§Ú©Ø«Ø± Ù…ÙˆØ§Ø±Ø¯ Ú©Ù…ØªØ± Ø§Ø² Û²Ùª


WordRepo | Ù¾Ø§ÛŒÚ¯Ø§Ù‡ ÙˆØ§Ú˜Ú¯Ø§Ù†
========================

.. tab-set::

    .. tab-item:: English

        **WordRepo** manages the database of foreign words and their Persian equivalents. You can access, search, add, remove, or update words.

        *Constructor*

        .. code-block:: python

            from pasban.db import WordRepo

            repo = WordRepo()

        *Methods*

        .. py:method:: get_all_words(reload: bool = False) -> dict[str, str]

            Get all words and their Persian equivalents.

            :param reload: Force reload from database (default: False)
            :return: Dictionary mapping foreign words to Persian equivalents

        .. py:method:: search_word(search_term: str, limit: int = 5) -> list[tuple[str, str]]

            Search for a word or its Persian equivalent.

            :param search_term: Term to search for
            :param limit: Maximum number of results (default: 5)
            :return: List of tuples (foreign_word, persian_equivalent)

        .. py:method:: add_word(foreign: str, persian: str) -> None

            Add a new word to the database.

            :param foreign: Foreign (non-Persian) word
            :param persian: Persian equivalent

        .. py:method:: remove_word(foreign: str) -> None

            Remove a word from the database.

            :param foreign: Foreign word to remove

        .. py:method:: update_word(foreign: str, persian: str) -> None

            Update a word's Persian equivalent.

            :param foreign: Foreign word to update
            :param persian: New Persian equivalent

        .. py:method:: get_persian(foreign: str) -> str

            Get the Persian equivalent of a foreign word.

            :param foreign: Foreign word
            :return: Persian equivalent (or empty string if not found)

        *Example Usage*

        .. code-block:: python

            from pasban.db import WordRepo

            repo = WordRepo()

            # Get all words
            all_words = repo.get_all_words()
            print(f"Total words: {len(all_words)}")
            print(f"Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ± -> {all_words.get('Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±')}")
            # Output: Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ± -> Ø±Ø§ÛŒØ§Ù†Ù‡

            # Search for a word
            results = repo.search_word("Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±", limit=5)
            for foreign, persian in results:
                print(f"{foreign} -> {persian}")
            # Output: Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ± -> Ø±Ø§ÛŒØ§Ù†Ù‡

            # Get Persian equivalent
            persian = repo.get_persian("Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±")
            print(persian)
            # Output: Ø±Ø§ÛŒØ§Ù†Ù‡

            # Add a new word
            repo.add_word("Ø§ÛŒÙ…ÛŒÙ„", "Ø±Ø§ÛŒØ§Ù†Ø§Ù…Ù‡")
            print(repo.get_persian("Ø§ÛŒÙ…ÛŒÙ„"))
            # Output: Ø±Ø§ÛŒØ§Ù†Ø§Ù…Ù‡

            # Update a word
            repo.update_word("Ø§ÛŒÙ…ÛŒÙ„", "Ù¾Ø³Øª Ø§Ù„Ú©ØªØ±ÙˆÙ†ÛŒÚ©")
            print(repo.get_persian("Ø§ÛŒÙ…ÛŒÙ„"))
            # Output: Ù¾Ø³Øª Ø§Ù„Ú©ØªØ±ÙˆÙ†ÛŒÚ©

            # Remove a word
            repo.remove_word("Ø§ÛŒÙ…ÛŒÙ„")
            print(repo.get_persian("Ø§ÛŒÙ…ÛŒÙ„"))
            # Output: (empty string)

            # Search with Persian equivalent
            results = repo.search_word("Ø±Ø§ÛŒØ§Ù†Ù‡")
            for foreign, persian in results:
                print(f"{foreign} <- {persian}")
    .. tab-item:: Ù¾Ø§Ø±Ø³ÛŒ

        **WordRepo** Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡Ù” ÙˆØ§Ú˜Ú¯Ø§Ù† Ø¨ÛŒÚ¯Ø§Ù†Ù‡ Ùˆ Ø¨Ø±Ø§Ø¨Ø± Ù¾Ø§Ø±Ø³ÛŒ Ø¢Ù†â€ŒÙ‡Ø§ Ø±Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø¨Ù‡ ÙˆØ§Ú˜Ú¯Ø§Ù† Ø¯Ø³ØªØ±Ø³ÛŒ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒØ¯ØŒ Ø¬Ø³ØªØ¬Ùˆ Ú©Ù†ÛŒØ¯ØŒ Ø¨ÛŒÙØ²Ø§ÛŒÛŒØ¯ØŒ Ø¨Ø±Ø¯Ø§Ø±ÛŒØ¯ ÛŒØ§ ÙˆÛŒØ±Ø§ÛŒØ´ Ú©Ù†ÛŒØ¯.

        *Ø³Ø§Ø²Ù†Ø¯Ù‡*

        .. code-block:: python

            from pasban.db import WordRepo

            repo = WordRepo()

        *Ù…ØªØ¯Ù‡Ø§*

        .. py:method:: get_all_words(reload: bool = False) -> dict[str, str]
            :no-index:

            Ø¯Ø±ÛŒØ§ÙØª Ù‡Ù…Ù‡Ù” ÙˆØ§Ú˜Ú¯Ø§Ù† Ùˆ Ø¨Ø±Ø§Ø¨Ø± Ù¾Ø§Ø±Ø³ÛŒ Ø¢Ù†â€ŒÙ‡Ø§.

            :param reload: Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø§Ø¬Ø¨Ø§Ø±ÛŒ Ø§Ø² Ù¾Ø§ÛŒÚ¯Ø§Ù‡â€ŒØ¯Ø§Ø¯Ù‡ (Ù¾ÛŒØ´â€ŒÙØ±Ø¶: False)
            :return: Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ù†Ú¯Ø§Ø´Øª ÙˆØ§Ú˜Ú¯Ø§Ù† Ø¨ÛŒÚ¯Ø§Ù†Ù‡ Ø¨Ù‡ Ø¨Ø±Ø§Ø¨Ø± Ù¾Ø§Ø±Ø³ÛŒ

        .. py:method:: search_word(search_term: str, limit: int = 5) -> list[tuple[str, str]]
            :no-index:

            Ø¬Ø³ØªØ¬ÙˆÛŒ ÙˆØ§Ú˜Ù‡ ÛŒØ§ Ø¨Ø±Ø§Ø¨Ø± Ù¾Ø§Ø±Ø³ÛŒ Ø¢Ù†.

            :param search_term: Ø¹Ø¨Ø§Ø±Øª Ø¬Ø³ØªØ¬Ùˆ
            :param limit: Ø­Ø¯Ø§Ú©Ø«Ø± ØªØ¹Ø¯Ø§Ø¯ Ù†ØªØ§ÛŒØ¬ (Ù¾ÛŒØ´â€ŒÙØ±Ø¶: 5)
            :return: ÙÙ‡Ø±Ø³Øª ØªØ§Ù¾Ù„â€ŒÙ‡Ø§ (ÙˆØ§Ú˜Ù‡_Ø¨ÛŒÚ¯Ø§Ù†Ù‡ØŒ Ø¨Ø±Ø§Ø¨Ø±_Ù¾Ø§Ø±Ø³ÛŒ)

        .. py:method:: add_word(foreign: str, persian: str) -> None
            :no-index:

            Ø§ÙØ²ÙˆØ¯Ù† ÙˆØ§Ú˜Ù‡Ù” ØªØ§Ø²Ù‡ Ø¨Ù‡ Ù¾Ø§ÛŒÚ¯Ø§Ù‡â€ŒØ¯Ø§Ø¯Ù‡.

            :param foreign: ÙˆØ§Ú˜Ù‡ Ø¨ÛŒÚ¯Ø§Ù†Ù‡ (ØºÛŒØ±Ù¾Ø§Ø±Ø³ÛŒ)
            :param persian: Ø¨Ø±Ø§Ø¨Ø± Ù¾Ø§Ø±Ø³ÛŒ

        .. py:method:: remove_word(foreign: str) -> None
            :no-index:

            Ø¨Ø±Ø¯Ø§Ø´ØªÙ† ÙˆØ§Ú˜Ù‡ Ø§Ø² Ù¾Ø§ÛŒÚ¯Ø§Ù‡â€ŒØ¯Ø§Ø¯Ù‡.

            :param foreign: ÙˆØ§Ú˜Ù‡ Ø¨ÛŒÚ¯Ø§Ù†Ù‡ Ø¨Ø±Ø§ÛŒ Ø­Ø°Ù

        .. py:method:: update_word(foreign: str, persian: str) -> None
            :no-index:

            ÙˆÛŒØ±Ø§ÛŒØ´ Ø¨Ø±Ø§Ø¨Ø± Ù¾Ø§Ø±Ø³ÛŒ ÛŒÚ© ÙˆØ§Ú˜Ù‡.

            :param foreign: ÙˆØ§Ú˜Ù‡ Ø¨ÛŒÚ¯Ø§Ù†Ù‡ Ø¨Ø±Ø§ÛŒ ÙˆÛŒØ±Ø§ÛŒØ´
            :param persian: Ø¨Ø±Ø§Ø¨Ø± Ù¾Ø§Ø±Ø³ÛŒ Ø¬Ø¯ÛŒØ¯

        .. py:method:: get_persian(foreign: str) -> str
            :no-index:

            Ø¯Ø±ÛŒØ§ÙØª Ø¨Ø±Ø§Ø¨Ø± Ù¾Ø§Ø±Ø³ÛŒ ÛŒÚ© ÙˆØ§Ú˜Ù‡ Ø¨ÛŒÚ¯Ø§Ù†Ù‡.

            :param foreign: ÙˆØ§Ú˜Ù‡ Ø¨ÛŒÚ¯Ø§Ù†Ù‡
            :return: Ø¨Ø±Ø§Ø¨Ø± Ù¾Ø§Ø±Ø³ÛŒ (ÛŒØ§ Ø±Ø´ØªÙ‡ Ø®Ø§Ù„ÛŒ Ø¯Ø± ØµÙˆØ±Øª Ù†ÛŒØ§ÙØªÙ†)

        *Ù†Ù…ÙˆÙ†Ù‡ Ú©Ø§Ø±Ø¨Ø±Ø¯*

        .. code-block:: python

            from pasban.db import WordRepo

            repo = WordRepo()

            # Ø¯Ø±ÛŒØ§ÙØª Ù‡Ù…Ù‡ ÙˆØ§Ú˜Ú¯Ø§Ù†
            all_words = repo.get_all_words()
            print(f"ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ ÙˆØ§Ú˜Ú¯Ø§Ù†: {len(all_words)}")
            print(f"Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ± -> {all_words.get('Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±')}")
            # Ø®Ø±ÙˆØ¬ÛŒ: Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ± -> Ø±Ø§ÛŒØ§Ù†Ù‡

            # Ø¬Ø³ØªØ¬ÙˆÛŒ ÙˆØ§Ú˜Ù‡
            results = repo.search_word("Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±", limit=5)
            for foreign, persian in results:
                print(f"{foreign} -> {persian}")
            # Ø®Ø±ÙˆØ¬ÛŒ: Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ± -> Ø±Ø§ÛŒØ§Ù†Ù‡

            # Ø¯Ø±ÛŒØ§ÙØª Ø¨Ø±Ø§Ø¨Ø± Ù¾Ø§Ø±Ø³ÛŒ
            persian = repo.get_persian("Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±")
            print(persian)
            # Ø®Ø±ÙˆØ¬ÛŒ: Ø±Ø§ÛŒØ§Ù†Ù‡

            # Ø§ÙØ²ÙˆØ¯Ù† ÙˆØ§Ú˜Ù‡ Ø¬Ø¯ÛŒØ¯
            repo.add_word("Ø§ÛŒÙ…ÛŒÙ„", "Ø±Ø§ÛŒØ§Ù†Ø§Ù…Ù‡")
            print(repo.get_persian("Ø§ÛŒÙ…ÛŒÙ„"))
            # Ø®Ø±ÙˆØ¬ÛŒ: Ø±Ø§ÛŒØ§Ù†Ø§Ù…Ù‡

            # ÙˆÛŒØ±Ø§ÛŒØ´ ÙˆØ§Ú˜Ù‡
            repo.update_word("Ø§ÛŒÙ…ÛŒÙ„", "Ù¾Ø³Øª Ø§Ù„Ú©ØªØ±ÙˆÙ†ÛŒÚ©")
            print(repo.get_persian("Ø§ÛŒÙ…ÛŒÙ„"))
            # Ø®Ø±ÙˆØ¬ÛŒ: Ù¾Ø³Øª Ø§Ù„Ú©ØªØ±ÙˆÙ†ÛŒÚ©

            # Ø­Ø°Ù ÙˆØ§Ú˜Ù‡
            repo.remove_word("Ø§ÛŒÙ…ÛŒÙ„")
            print(repo.get_persian("Ø§ÛŒÙ…ÛŒÙ„"))
            # Ø®Ø±ÙˆØ¬ÛŒ: (Ø±Ø´ØªÙ‡ Ø®Ø§Ù„ÛŒ)

            # Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø§ Ø¨Ø±Ø§Ø¨Ø± Ù¾Ø§Ø±Ø³ÛŒ
            results = repo.search_word("Ø±Ø§ÛŒØ§Ù†Ù‡")
            for foreign, persian in results:
                print(f"{foreign} <- {persian}")

        *Ú©Ø§Ø±Ø¨Ø±Ø¯ Ù¾ÛŒØ´Ø±ÙØªÙ‡*

        .. code-block:: python

            from pasban.db import WordRepo

            repo = WordRepo()

            # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ù¾Ø§ÛŒÚ¯Ø§Ù‡â€ŒØ¯Ø§Ø¯Ù‡ Ø§Ø² Ø¯ÛŒØ³Ú©
            all_words = repo.get_all_words(reload=True)

            # Ø¹Ù…Ù„ÛŒØ§Øª Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ
            new_words = {
                "ÙˆØ¨Ø³Ø§ÛŒØª": "ÙˆØ¨Ú¯Ø§Ù‡",
                "Ø§ÛŒÙ…ÛŒÙ„": "Ø±Ø§ÛŒØ§Ù†Ø§Ù…Ù‡",
                "ÙØ§ÛŒÙ„": "Ù¾Ø±ÙˆÙ†Ø¯Ù‡"
            }

            for foreign, persian in new_words.items():
                repo.add_word(foreign, persian)

            # Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø§ Ø¹Ø¨Ø§Ø±Øªâ€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù
            computer_words = repo.search_word("Ø±Ø§ÛŒØ§Ù†Ù‡", limit=10)
            print(f"{len(computer_words)} ÙˆØ§Ú˜Ù‡ Ù…Ø±ØªØ¨Ø· Ø¨Ø§ Ø±Ø§ÛŒØ§Ù†Ù‡ ÛŒØ§ÙØª Ø´Ø¯")

        .. code-block:: python

            from pasban.detector import WordDetector, WordDetectorRegex

            # Detect without normalization
            result = detector.detect(text, normalize=False)

            # Detect without contextual cleaning
            result = detector.detect(text, contextual=False)

            # Detect with both disabled (fastest, least accurate)
            result = detector.detect(text, normalize=False, contextual=False)

            # Compare both engines
            detector_ac = WordDetector()
            detector_re = WordDetectorRegex()

            result_ac = detector_ac.detect(text)
            result_re = detector_re.detect(text)

            print(f"Aho-Corasick found: {result_ac.count} words")
            print(f"Regex found: {result_re.count} words")

            # Reload database after updates
            detector.reload()

DataLoader | Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ ÙˆØ§Ú˜Ú¯Ø§Ù†
====================================

.. tab-set::

    .. tab-item:: English

        **DataLoader** handles downloading and updating the Pasban word database from GitHub.
        It ensures you always have the latest version and manages the local storage path.

        *Constructor & Usage*

        .. code-block:: python

            from pasban.db.loader import DataLoader

            # Initialize database (downloads if missing)
            DataLoader.initialize()

            # Get local database path
            db_path = DataLoader.get_db_path()
            print(db_path)

            # Check and update if needed
            DataLoader.update()

        *Methods*

        .. py:method:: initialize() -> None

            Ensure the database exists locally.
            If not, downloads the latest release automatically.

        .. py:method:: get_db_path() -> Path

            Get the path to the local database file.
            Automatically triggers initialization if missing.

            :return: Path object pointing to `pasban.db`

        .. py:method:: update(force_update: bool = False) -> None

            Check for updates and download the latest database if needed.

            :param force_update: If True, download the latest release even if
                                 the local version is up-to-date (default: False)

        .. py:method:: _get_lasted_tag() -> Optional[int]

            Internal method to read the last downloaded release tag from TAG file.

            :return: Last stored tag as integer, or None if unavailable

        .. py:method:: _get_release_data() -> dict

            Fetch metadata of the latest release from GitHub API.

            :return: JSON dictionary with release information

        .. py:method:: _get_db_url(assets_url: str) -> str

            Get the direct download URL for `pasban.db` from release assets.

            :param assets_url: GitHub API URL for release assets
            :return: Direct browser download URL
            :raises DatabaseNotFound: If `pasban.db` is not found

        .. py:method:: _download_release(assets_url: str, tag: str) -> None

            Download the latest database release and update the TAG file.

            :param assets_url: GitHub API URL for release assets
            :param tag: Release tag string

        *Example Usage*

        .. code-block:: python

            from pasban.db.loader import DataLoader

            # Force update the database
            DataLoader.update(force_update=True)

            # Normal update (only if new version available)
            DataLoader.update()

            # Get database path for other usage
            db_path = DataLoader.get_db_path()
            print(f"Database is stored at: {db_path}")

    .. tab-item:: Ù¾Ø§Ø±Ø³ÛŒ

        **Ø¨Ø§Ø±Ø¢ÙˆØ± Ø¯Ø§Ø¯Ù‡** Ù…Ø¯ÛŒØ±ÛŒØª Ø¯Ø§Ù†Ù„ÙˆØ¯ Ùˆ Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ ÙˆØ§Ú˜Ú¯Ø§Ù† Ù¾Ø§Ø³Ø¨Ø§Ù† Ø±Ø§ Ø¨Ø±Ø¹Ù‡Ø¯Ù‡ Ø¯Ø§Ø±Ø¯.
        Ø§ÛŒÙ† Ú©Ù„Ø§Ø³ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Ú©Ù‡ Ù‡Ù…ÛŒØ´Ù‡ ØªØ§Ø²Ù‡â€ŒØªØ±ÛŒÙ† Ù†Ø³Ø®Ù‡ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ Ø±ÙˆÛŒ Ø¯Ø³ØªÚ¯Ø§Ù‡ Ø´Ù…Ø§ Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ø´Ø¯ Ùˆ Ù…Ø³ÛŒØ± Ù†Ú¯Ù‡Ø¯Ø§Ø±ÛŒ Ù…Ø­Ù„ÛŒ Ø±Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ù…ÛŒâ€ŒÚ©Ù†Ø¯.

        *Ø³Ø§Ø²Ù†Ø¯Ù‡ Ùˆ Ù†Ù…ÙˆÙ†Ù‡ Ú©Ø§Ø±Ø¨Ø±Ø¯*

        .. code-block:: python

            from pasban.db.loader import DataLoader

            # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² ÙˆØ¬ÙˆØ¯ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ (Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø¯Ø± ØµÙˆØ±Øª Ù†Ø¨ÙˆØ¯)
            DataLoader.initialize()

            # Ø¯Ø±ÛŒØ§ÙØª Ù…Ø³ÛŒØ± Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡
            db_path = DataLoader.get_db_path()
            print(db_path)

            # Ø¨Ø±Ø±Ø³ÛŒ Ùˆ Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¯Ø± ØµÙˆØ±Øª Ù†ÛŒØ§Ø²
            DataLoader.update()

        *Ù…ØªØ¯Ù‡Ø§*

        .. py:method:: initialize() -> None
            :no-index:

            Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ù…ÙˆØ¬ÙˆØ¯ Ø¨ÙˆØ¯Ù† Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ Ø¨Ù‡â€ŒØµÙˆØ±Øª Ù…Ø­Ù„ÛŒ.
            Ø¯Ø± ØµÙˆØ±Øª Ù†Ø¨ÙˆØ¯ØŒ ØªØ§Ø²Ù‡â€ŒØªØ±ÛŒÙ† Ù†Ø³Ø®Ù‡ Ø±Ø§ Ø®ÙˆØ¯Ú©Ø§Ø± Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.

        .. py:method:: get_db_path() -> Path
            :no-index:

            Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ Ù…Ø­Ù„ÛŒ Ø±Ø§ Ø¨Ø§Ø²Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯.
            Ø¯Ø± ØµÙˆØ±Øª Ù†Ø¨ÙˆØ¯ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ØŒ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø§ÙˆÙ„ÛŒÙ‡ Ø§Ø¬Ø±Ø§ Ù…ÛŒâ€ŒØ´ÙˆØ¯.

            :return: Ø´ÛŒØ¡ Path Ú©Ù‡ Ø¨Ù‡ `pasban.db` Ø§Ø´Ø§Ø±Ù‡ Ø¯Ø§Ø±Ø¯

        .. py:method:: update(force_update: bool = False) -> None
            :no-index:

            Ø¨Ø±Ø±Ø³ÛŒ Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒâ€ŒÙ‡Ø§ Ùˆ Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ ØªØ§Ø²Ù‡â€ŒØªØ±ÛŒÙ† Ù†Ø³Ø®Ù‡ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ Ø¯Ø± ØµÙˆØ±Øª Ù†ÛŒØ§Ø².

            :param force_update: Ø§Ú¯Ø± True Ø¨Ø§Ø´Ø¯ØŒ Ù‡Ù…ÛŒØ´Ù‡ ØªØ§Ø²Ù‡â€ŒØªØ±ÛŒÙ† Ù†Ø³Ø®Ù‡ Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ Ù…ÛŒâ€ŒØ´ÙˆØ¯
                                 Ø­ØªÛŒ Ø§Ú¯Ø± Ù†Ø³Ø®Ù‡ Ù…Ø­Ù„ÛŒ Ø¨Ù‡â€ŒØ±ÙˆØ² Ø¨Ø§Ø´Ø¯ (Ù¾ÛŒØ´â€ŒÙØ±Ø¶: False)

        .. py:method:: _get_lasted_tag() -> Optional[int]
            :no-index:

            Ú©Ø±Ø¯Ø§Ø± Ø¯Ø±ÙˆÙ†ÛŒ Ø¨Ø±Ø§ÛŒ Ø®ÙˆØ§Ù†Ø¯Ù† Ø¢Ø®Ø±ÛŒÙ† Ø´Ù…Ø§Ø±Ù‡ Ù†Ø³Ø®Ù‡ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø´Ø¯Ù‡ Ø§Ø² Ù¾Ø±ÙˆÙ†Ø¯Ù‡ TAG.

            :return: Ø¢Ø®Ø±ÛŒÙ† Ø´Ù…Ø§Ø±Ù‡ Ù†Ø³Ø®Ù‡ Ø¨Ù‡â€ŒØµÙˆØ±Øª Ø¹Ø¯Ø¯ ÛŒØ§ None Ø¯Ø± ØµÙˆØ±Øª Ù†Ø¨ÙˆØ¯

        .. py:method:: _get_release_data() -> dict
            :no-index:

            Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ø³Ø®Ù‡ ØªØ§Ø²Ù‡ Ø§Ø² Ø³Ø±ÙˆÛŒØ³ GitHub.

            :return: Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ JSON Ø´Ø§Ù…Ù„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ø³Ø®Ù‡

        .. py:method:: _get_db_url(assets_url: str) -> str
            :no-index:

            Ø¯Ø±ÛŒØ§ÙØª Ù†Ø´Ø§Ù†ÛŒ Ù…Ø³ØªÙ‚ÛŒÙ… Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ `pasban.db` Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ø³Ø®Ù‡.

            :param assets_url: Ù†Ø´Ø§Ù†ÛŒ API Ú¯ÛŒØªâ€ŒÙ‡Ø§Ø¨ Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ø³Ø®Ù‡
            :return: Ù†Ø´Ø§Ù†ÛŒ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…Ø³ØªÙ‚ÛŒÙ…
            :raises DatabaseNotFound: Ø¯Ø± ØµÙˆØ±Øª Ù†Ø¨ÙˆØ¯ Ù¾Ø±ÙˆÙ†Ø¯Ù‡ `pasban.db`

        .. py:method:: _download_release(assets_url: str, tag: str) -> None
            :no-index:

            Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ ØªØ§Ø²Ù‡â€ŒØªØ±ÛŒÙ† Ù†Ø³Ø®Ù‡ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ Ùˆ Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù¾Ø±ÙˆÙ†Ø¯Ù‡ TAG.

            :param assets_url: Ù†Ø´Ø§Ù†ÛŒ API Ú¯ÛŒØªâ€ŒÙ‡Ø§Ø¨ Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
            :param tag: Ø´Ù…Ø§Ø±Ù‡ Ù†Ø³Ø®Ù‡

        *Ù†Ù…ÙˆÙ†Ù‡ Ú©Ø§Ø±Ø¨Ø±Ø¯*

        .. code-block:: python

            from pasban.db.loader import DataLoader

            # Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø§Ø¬Ø¨Ø§Ø±ÛŒ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡
            DataLoader.update(force_update=True)

            # Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù…Ø¹Ù…ÙˆÙ„ÛŒ (ØªÙ†Ù‡Ø§ Ø¯Ø± ØµÙˆØ±Øª ÙˆØ¬ÙˆØ¯ Ù†Ø³Ø®Ù‡ ØªØ§Ø²Ù‡)
            DataLoader.update()

            # Ø¯Ø±ÛŒØ§ÙØª Ù…Ø³ÛŒØ± Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø±Ø¯Ù‡Ø§ÛŒ Ø¯ÛŒÚ¯Ø±
            db_path = DataLoader.get_db_path()
            print(f"Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ Ø¯Ø± Ù…Ø³ÛŒØ±: {db_path}")

Normalizer | Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²
========================

.. tab-set::

    .. tab-item:: English

        Normalize Persian text and remove non-standard characters and punctuation. The normalizer converts Arabic characters to Persian equivalents and ensures consistent text representation.

        *Methods*

        .. py:method:: WordNormalizer.normalize_text(text: str) -> str

            Normalize Persian text by converting Arabic characters to Persian equivalents and removing non-standard characters.

            :param text: Input text to normalize
            :return: Normalized text

            **Normalizations applied:**

            * Arabic Ùƒ (U+0643) â†’ Persian Ú© (U+06A9)
            * Arabic ÙŠ (U+064A) â†’ Persian ÛŒ (U+06CC)
            * Arabic Ø© (U+0629) â†’ Persian Ù‡ (U+0647)
            * Zero-width characters removed
            * Diacritics removed
            * Multiple spaces collapsed to single space

        *Example Usage*

        .. code-block:: python

            from pasban.normalizer.text_normalizer import WordNormalizer

            # Basic normalization
            text = "Ø§ÛŒÙ† Ù…ØªÙ† Ø´Ø§Ù…Ù„ ÙƒØŒ ÙŠ Ùˆ Ø© Ø§Ø³Øª!"
            normalized = WordNormalizer.normalize_text(text)
            print(normalized)
            # Output: "Ø§ÛŒÙ† Ù…ØªÙ† Ø´Ø§Ù…Ù„ Ú© ÛŒ Ù‡ Ø§Ø³Øª"

            # Normalize mixed text
            mixed_text = "ÙƒØªØ§Ø¨    Ø¯Ø±    ÙƒØªØ§Ø¨Ø®Ø§Ù†Ù‡    Ø§Ø³Øª"
            normalized = WordNormalizer.normalize_text(mixed_text)
            print(normalized)
            # Output: "Ú©ØªØ§Ø¨ Ø¯Ø± Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ Ø§Ø³Øª"

            # Remove diacritics
            text_with_diacritics = "Ù…ÙØ«ÙÙ„Ø§Ù‹ Ú©ÙØªØ§Ø¨Ù Ø®ÙˆØ¨ÛŒ Ø§Ø³Øª"
            normalized = WordNormalizer.normalize_text(text_with_diacritics)
            print(normalized)
            # Output: "Ù…Ø«Ù„Ø§ Ú©ØªØ§Ø¨ Ø®ÙˆØ¨ÛŒ Ø§Ø³Øª"

        .. admonition:: When to use normalization
            :class: tip

            * Before processing any Persian text
            * When comparing Persian strings
            * Before storing text in databases
            * When preparing text for machine learning
            * Recommended to always use with detectors (enabled by default)

    .. tab-item:: Ù¾Ø§Ø±Ø³ÛŒ

        Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù…ØªÙ† Ù¾Ø§Ø±Ø³ÛŒ Ùˆ Ø²Ø¯ÙˆØ¯Ù† Ù†ÙˆÛŒØ³Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨ÛŒÚ¯Ø§Ù†Ù‡ Ùˆ Ù†Ø´Ø§Ù†Ù‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ. Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø² Ù†ÙˆÛŒØ³Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø²ÛŒ Ø±Ø§ Ø¨Ù‡ Ø¨Ø±Ø§Ø¨Ø± Ù¾Ø§Ø±Ø³ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ùˆ Ø¨Ø§Ø²Ù†Ù…Ø§ÛŒÛŒ ÛŒÚ©Ø³Ø§Ù† Ù…ØªÙ† Ø±Ø§ ØªØ¶Ù…ÛŒÙ† Ù…ÛŒâ€ŒÚ©Ù†Ø¯.

        *Ù…ØªØ¯Ù‡Ø§*

        .. py:method:: WordNormalizer.normalize_text(text: str) -> str
            :no-index:

            Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù…ØªÙ† Ù¾Ø§Ø±Ø³ÛŒ Ø¨Ø§ ØªØ¨Ø¯ÛŒÙ„ Ù†ÙˆÛŒØ³Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø²ÛŒ Ø¨Ù‡ Ø¨Ø±Ø§Ø¨Ø± Ù¾Ø§Ø±Ø³ÛŒ Ùˆ Ø­Ø°Ù Ù†ÙˆÛŒØ³Ù‡â€ŒÙ‡Ø§ÛŒ ØºÛŒØ±Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯.

            :param text: Ù…ØªÙ† ÙˆØ±ÙˆØ¯ÛŒ Ø¨Ø±Ø§ÛŒ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ
            :return: Ù…ØªÙ† Ù†Ø±Ù…Ø§Ù„â€ŒØ´Ø¯Ù‡

            **Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ø¹Ù…Ø§Ù„â€ŒØ´Ø¯Ù‡:**

            * Ùƒ ØªØ§Ø²ÛŒ (U+0643) â† Ú© Ù¾Ø§Ø±Ø³ÛŒ (U+06A9)
            * ÙŠ ØªØ§Ø²ÛŒ (U+064A) â† ÛŒ Ù¾Ø§Ø±Ø³ÛŒ (U+06CC)
            * Ø© ØªØ§Ø²ÛŒ (U+0629) â† Ù‡ Ù¾Ø§Ø±Ø³ÛŒ (U+0647)
            * Ø­Ø°Ù Ù†ÙˆÛŒØ³Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ù‡Ù†Ø§ÛŒ ØµÙØ±
            * Ø­Ø°Ù Ø§Ø¹Ø±Ø§Ø¨
            * ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§ÛŒ Ú†Ù†Ø¯Ú¯Ø§Ù†Ù‡ Ø¨Ù‡ ÛŒÚ© ÙØ§ØµÙ„Ù‡

        *Ù†Ù…ÙˆÙ†Ù‡ Ú©Ø§Ø±Ø¨Ø±Ø¯*

        .. code-block:: python

            from pasban.normalizer.text_normalizer import WordNormalizer

            # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø³Ø§Ø¯Ù‡
            text = "Ø§ÛŒÙ† Ù…ØªÙ† Ø´Ø§Ù…Ù„ ÙƒØŒ ÙŠ Ùˆ Ø© Ø§Ø³Øª!"
            normalized = WordNormalizer.normalize_text(text)
            print(normalized)
            # Ø®Ø±ÙˆØ¬ÛŒ: "Ø§ÛŒÙ† Ù…ØªÙ† Ø´Ø§Ù…Ù„ Ú© ÛŒ Ù‡ Ø§Ø³Øª"

            # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù…ØªÙ† Ù…Ø®ØªÙ„Ø·
            mixed_text = "ÙƒØªØ§Ø¨    Ø¯Ø±    ÙƒØªØ§Ø¨Ø®Ø§Ù†Ù‡    Ø§Ø³Øª"
            normalized = WordNormalizer.normalize_text(mixed_text)
            print(normalized)
            # Ø®Ø±ÙˆØ¬ÛŒ: "Ú©ØªØ§Ø¨ Ø¯Ø± Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ Ø§Ø³Øª"

            # Ø­Ø°Ù Ø§Ø¹Ø±Ø§Ø¨
            text_with_diacritics = "Ù…ÙØ«ÙÙ„Ø§Ù‹ Ú©ÙØªØ§Ø¨Ù Ø®ÙˆØ¨ÛŒ Ø§Ø³Øª"
            normalized = WordNormalizer.normalize_text(text_with_diacritics)
            print(normalized)
            # Ø®Ø±ÙˆØ¬ÛŒ: "Ù…Ø«Ù„Ø§ Ú©ØªØ§Ø¨ Ø®ÙˆØ¨ÛŒ Ø§Ø³Øª"

        .. admonition:: Ú†Ù‡ Ø²Ù…Ø§Ù†ÛŒ Ø§Ø² Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒÙ…
            :class: tip

            * Ù¾ÛŒØ´ Ø§Ø² Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù‡Ø±Ú¯ÙˆÙ†Ù‡ Ù…ØªÙ† Ù¾Ø§Ø±Ø³ÛŒ
            * Ù‡Ù†Ú¯Ø§Ù… Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø±Ø´ØªÙ‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø§Ø±Ø³ÛŒ
            * Ù¾ÛŒØ´ Ø§Ø² Ø°Ø®ÛŒØ±Ù‡ Ù…ØªÙ† Ø¯Ø± Ù¾Ø§ÛŒÚ¯Ø§Ù‡â€ŒØ¯Ø§Ø¯Ù‡
            * Ù‡Ù†Ú¯Ø§Ù… Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…ØªÙ† Ø¨Ø±Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ†
            * ØªÙˆØµÛŒÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ù‡Ù…ÙˆØ§Ø±Ù‡ Ø¨Ø§ Ø´Ù†Ø§Ø³Ø§Ú¯Ø±Ù‡Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´ÙˆØ¯ (Ù¾ÛŒØ´â€ŒÙØ±Ø¶ ÙØ¹Ø§Ù„ Ø§Ø³Øª)


Contextual Cleaner | Ù¾Ø§Ù„Ø§ÛŒØ´Ú¯Ø± Ø²Ù…ÛŒÙ†Ù‡â€ŒØ§ÛŒ
========================================

.. tab-set::

    .. tab-item:: English

        Remove contextual patterns and special combinations from text. The contextual cleaner identifies and removes Persian name patterns, common word combinations, and other context-specific patterns that should not be flagged as foreign words.

        *Methods*

        .. py:method:: contextual_cleaner.clean_text(text: str) -> str

            Remove contextual patterns from text.

            :param text: Input text to clean
            :return: Cleaned text

            **Patterns removed:**

            * Persian full names (first name + last name)
            * Common Persian compound words
            * Persian idioms and expressions
            * Proper nouns with Persian markers
            * Date and time expressions

        *Example Usage*

        .. code-block:: python

            from pasban.normalizer.contextual_remover import contextual_cleaner

            # Remove name patterns
            text = "Ø­Ø§ÙØ¸ Ø´ÛŒØ±Ø§Ø²ÛŒ Ø´Ø§Ø¹Ø± Ù†Ø§Ù…Ø¯Ø§Ø± Ø§Ø³Øª."
            cleaned = contextual_cleaner.clean_text(text)
            print(cleaned)
            # Names like "Ø­Ø§ÙØ¸ Ø´ÛŒØ±Ø§Ø²ÛŒ" are removed from detection

            # Remove compound words
            text = "Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡Ù” Ù…Ù„ÛŒ Ø§ÛŒØ±Ø§Ù†"
            cleaned = contextual_cleaner.clean_text(text)
            print(cleaned)

            # Complex example with multiple patterns
            text = """
            Ø±ÙˆÙ…ÛŒ Ù…ÙˆÙ„Ø§Ù†Ø§ Ø¬Ù„Ø§Ù„â€ŒØ§Ù„Ø¯ÛŒÙ† Ù…Ø­Ù…Ø¯ Ø¨Ù„Ø®ÛŒ Ø´Ø§Ø¹Ø± Ùˆ Ø¹Ø§Ø±Ù Ø¨Ø²Ø±Ú¯ Ø§ÛŒØ±Ø§Ù†ÛŒ
            Ø¯Ø± Ù‚Ø±Ù† Ù‡ÙØªÙ… Ù‡Ø¬Ø±ÛŒ Ø¯Ø± Ø´Ù‡Ø± Ø¨Ù„Ø® Ù…ØªÙˆÙ„Ø¯ Ø´Ø¯.
            """
            cleaned = contextual_cleaner.clean_text(text)
            print(cleaned)

        *Integration with Detector*

        .. code-block:: python

            from pasban.detector import WordDetector

            detector = WordDetector()

            text = "Ø­Ø§ÙØ¸ Ø´ÛŒØ±Ø§Ø²ÛŒ Ùˆ Ù…ÙˆÙ„Ø§Ù†Ø§ Ø±ÙˆÙ…ÛŒ Ø´Ø§Ø¹Ø±Ø§Ù† Ø¨Ø²Ø±Ú¯ Ø§ÛŒØ±Ø§Ù†ÛŒ Ù‡Ø³ØªÙ†Ø¯."

            # With contextual cleaning (default)
            result = detector.detect(text, contextual=True)
            print(f"With cleaning: {result.count} foreign words")

            # Without contextual cleaning
            result = detector.detect(text, contextual=False)
            print(f"Without cleaning: {result.count} foreign words")

        .. admonition:: When to use contextual cleaning
            :class: tip

            * When processing literary or historical texts
            * When text contains many Persian names
            * When working with formal Persian writing
            * Recommended for most use cases (enabled by default)
            * Disable for maximum detection sensitivity

    .. tab-item:: Ù¾Ø§Ø±Ø³ÛŒ

        Ø²Ø¯ÙˆØ¯Ù† Ø³Ø§Ø®ØªØ§Ø±Ù‡Ø§ÛŒ Ø²Ù…ÛŒÙ†Ù‡â€ŒØ§ÛŒ Ùˆ ØªØ±Ú©ÛŒØ¨â€ŒÙ‡Ø§ÛŒ Ø®Ø§Øµ Ø§Ø² Ù…ØªÙ†. Ù¾Ø§Ù„Ø§ÛŒØ´Ú¯Ø± Ø²Ù…ÛŒÙ†Ù‡â€ŒØ§ÛŒ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù†Ø§Ù… Ù¾Ø§Ø±Ø³ÛŒØŒ ØªØ±Ú©ÛŒØ¨â€ŒÙ‡Ø§ÛŒ Ø±Ø§ÛŒØ¬ ÙˆØ§Ú˜Ú¯Ø§Ù† Ùˆ Ø¯ÛŒÚ¯Ø± Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ ÙˆØ§Ø¨Ø³ØªÙ‡ Ø¨Ù‡ Ø²Ù…ÛŒÙ†Ù‡ Ø±Ø§ Ú©Ù‡ Ù†Ø¨Ø§ÛŒØ¯ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ÙˆØ§Ú˜Ù‡ Ø¨ÛŒÚ¯Ø§Ù†Ù‡ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´ÙˆÙ†Ø¯ØŒ Ù…ÛŒâ€ŒØ´Ù†Ø§Ø³Ø¯ Ùˆ Ø­Ø°Ù Ù…ÛŒâ€ŒÚ©Ù†Ø¯.

        *Ù…ØªØ¯Ù‡Ø§*

        .. py:method:: contextual_cleaner.clean_text(text: str) -> str
            :no-index:

            Ø²Ø¯ÙˆØ¯Ù† Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø²Ù…ÛŒÙ†Ù‡â€ŒØ§ÛŒ Ø§Ø² Ù…ØªÙ†.

            :param text: Ù…ØªÙ† ÙˆØ±ÙˆØ¯ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ù„Ø§ÛŒØ´
            :return: Ù…ØªÙ† Ù¾Ø§Ù„Ø§ÛŒØ´â€ŒØ´Ø¯Ù‡

            **Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø­Ø°Ùâ€ŒØ´Ø¯Ù‡:**

            * Ù†Ø§Ù…â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ù…Ù„ Ù¾Ø§Ø±Ø³ÛŒ (Ù†Ø§Ù… + Ù†Ø§Ù… Ø®Ø§Ù†ÙˆØ§Ø¯Ú¯ÛŒ)
            * ÙˆØ§Ú˜Ú¯Ø§Ù† Ù…Ø±Ú©Ø¨ Ø±Ø§ÛŒØ¬ Ù¾Ø§Ø±Ø³ÛŒ
            * Ø§ØµØ·Ù„Ø§Ø­Ø§Øª Ùˆ Ø¹Ø¨Ø§Ø±Ø§Øª Ù¾Ø§Ø±Ø³ÛŒ
            * Ø§Ø³Ø§Ù…ÛŒ Ø®Ø§Øµ Ø¨Ø§ Ù†Ø´Ø§Ù†Ú¯Ø±Ù‡Ø§ÛŒ Ù¾Ø§Ø±Ø³ÛŒ
            * Ø¹Ø¨Ø§Ø±Ø§Øª ØªØ§Ø±ÛŒØ® Ùˆ Ø²Ù…Ø§Ù†

        *Ù†Ù…ÙˆÙ†Ù‡ Ú©Ø§Ø±Ø¨Ø±Ø¯*

        .. code-block:: python

            from pasban.normalizer.contextual_remover import contextual_cleaner

            # Ø­Ø°Ù Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù†Ø§Ù…
            text = "Ø­Ø§ÙØ¸ Ø´ÛŒØ±Ø§Ø²ÛŒ Ø´Ø§Ø¹Ø± Ù†Ø§Ù…Ø¯Ø§Ø± Ø§Ø³Øª."
            cleaned = contextual_cleaner.clean_text(text)
            print(cleaned)
            # Ù†Ø§Ù…â€ŒÙ‡Ø§ÛŒÛŒ Ù…Ø§Ù†Ù†Ø¯ "Ø­Ø§ÙØ¸ Ø´ÛŒØ±Ø§Ø²ÛŒ" Ø§Ø² Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø­Ø°Ù Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯

            # Ø­Ø°Ù ÙˆØ§Ú˜Ú¯Ø§Ù† Ù…Ø±Ú©Ø¨
            text = "Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡Ù” Ù…Ù„ÛŒ Ø§ÛŒØ±Ø§Ù†"
            cleaned = contextual_cleaner.clean_text(text)
            print(cleaned)

            # Ù†Ù…ÙˆÙ†Ù‡Ù” Ù¾ÛŒÚ†ÛŒØ¯Ù‡ Ø¨Ø§ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ú†Ù†Ø¯Ú¯Ø§Ù†Ù‡
            text = """
            Ø±ÙˆÙ…ÛŒ Ù…ÙˆÙ„Ø§Ù†Ø§ Ø¬Ù„Ø§Ù„â€ŒØ§Ù„Ø¯ÛŒÙ† Ù…Ø­Ù…Ø¯ Ø¨Ù„Ø®ÛŒ Ø´Ø§Ø¹Ø± Ùˆ Ø¹Ø§Ø±Ù Ø¨Ø²Ø±Ú¯ Ø§ÛŒØ±Ø§Ù†ÛŒ
            Ø¯Ø± Ù‚Ø±Ù† Ù‡ÙØªÙ… Ù‡Ø¬Ø±ÛŒ Ø¯Ø± Ø´Ù‡Ø± Ø¨Ù„Ø® Ù…ØªÙˆÙ„Ø¯ Ø´Ø¯.
            """
            cleaned = contextual_cleaner.clean_text(text)
            print(cleaned)

        *ÛŒÚ©Ù¾Ø§Ø±Ú†Ú¯ÛŒ Ø¨Ø§ Ø´Ù†Ø§Ø³Ø§Ú¯Ø±*

        .. code-block:: python

            from pasban.detector import WordDetector

            detector = WordDetector()

            text = "Ø­Ø§ÙØ¸ Ø´ÛŒØ±Ø§Ø²ÛŒ Ùˆ Ù…ÙˆÙ„Ø§Ù†Ø§ Ø±ÙˆÙ…ÛŒ Ø´Ø§Ø¹Ø±Ø§Ù† Ø¨Ø²Ø±Ú¯ Ø§ÛŒØ±Ø§Ù†ÛŒ Ù‡Ø³ØªÙ†Ø¯."

            # Ø¨Ø§ Ù¾Ø§Ù„Ø§ÛŒØ´ Ø²Ù…ÛŒÙ†Ù‡â€ŒØ§ÛŒ (Ù¾ÛŒØ´â€ŒÙØ±Ø¶)
            result = detector.detect(text, contextual=True)
            print(f"Ø¨Ø§ Ù¾Ø§Ù„Ø§ÛŒØ´: {result.count} ÙˆØ§Ú˜Ù‡ Ø¨ÛŒÚ¯Ø§Ù†Ù‡")

            # Ø¨Ø¯ÙˆÙ† Ù¾Ø§Ù„Ø§ÛŒØ´ Ø²Ù…ÛŒÙ†Ù‡â€ŒØ§ÛŒ
            result = detector.detect(text, contextual=False)
            print(f"Ø¨Ø¯ÙˆÙ† Ù¾Ø§Ù„Ø§ÛŒØ´: {result.count} ÙˆØ§Ú˜Ù‡ Ø¨ÛŒÚ¯Ø§Ù†Ù‡")

        .. admonition:: Ú†Ù‡ Ø²Ù…Ø§Ù†ÛŒ Ø§Ø² Ù¾Ø§Ù„Ø§ÛŒØ´ Ø²Ù…ÛŒÙ†Ù‡â€ŒØ§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒÙ…
            :class: tip

            * Ù‡Ù†Ú¯Ø§Ù… Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ Ø§Ø¯Ø¨ÛŒ ÛŒØ§ ØªØ§Ø±ÛŒØ®ÛŒ
            * Ø²Ù…Ø§Ù†ÛŒ Ú©Ù‡ Ù…ØªÙ† Ø´Ø§Ù…Ù„ Ù†Ø§Ù…â€ŒÙ‡Ø§ÛŒ Ù¾Ø§Ø±Ø³ÛŒ Ø²ÛŒØ§Ø¯ÛŒ Ø§Ø³Øª
            * Ù‡Ù†Ú¯Ø§Ù… Ú©Ø§Ø± Ø¨Ø§ Ù†ÙˆØ´ØªØ§Ø± Ø±Ø³Ù…ÛŒ Ù¾Ø§Ø±Ø³ÛŒ
            * Ø¨Ø±Ø§ÛŒ Ø§Ú©Ø«Ø± Ú©Ø§Ø±Ø¨Ø±Ø¯Ù‡Ø§ ØªÙˆØµÛŒÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ (Ù¾ÛŒØ´â€ŒÙØ±Ø¶ ÙØ¹Ø§Ù„ Ø§Ø³Øª)
            * Ø¨Ø±Ø§ÛŒ Ø­Ø³Ø§Ø³ÛŒØª Ø¨ÛŒØ´ÛŒÙ†Ù‡ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒØŒ ØºÛŒØ±ÙØ¹Ø§Ù„ Ú©Ù†ÛŒØ¯


Core Data Types | Ø§Ù†ÙˆØ§Ø¹ Ø¯Ø§Ø¯Ù‡ Ù¾Ø§ÛŒÙ‡
=================================

DetectData
----------

.. tab-set::

    .. tab-item:: English

        Container for detection results and statistics. This object provides comprehensive information about detected foreign words and text statistics.

        *Attributes*

        .. py:attribute:: foreign_words
            :type: list[str]

            List of detected foreign words (may contain duplicates for multiple occurrences)

        .. py:attribute:: words
            :type: dict[str, str]

            Mapping of unique foreign words to their Persian equivalents

        .. py:attribute:: text
            :type: str

            Original or processed input text

        .. py:attribute:: count
            :type: int

            Total number of detected foreign word occurrences

        .. py:attribute:: unique_count
            :type: int

            Number of unique detected foreign words

        .. py:attribute:: total_words
            :type: int

            Total number of words in the text

        .. py:attribute:: foreign_percentage
            :type: float

            Percentage of foreign words in the text

        .. py:attribute:: to_text
            :type: str

            Detailed Persian text report

        .. py:attribute:: to_summary_text
            :type: str

            Concise Persian summary report

        *Example Usage*

        .. code-block:: python

            from pasban.detector import WordDetector

            detector = WordDetector()
            text = "Ø§ÛŒÙ† Ù…ØªÙ† Ø´Ø§Ù…Ù„ Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ± Ùˆ Ø§ÛŒÙ†ØªØ±Ù†Øª Ùˆ Ø³ÛŒØ³ØªÙ… Ø§Ø³Øª."
            result = detector.detect(text)

            # Access basic information
            print(f"Foreign words list: {result.foreign_words}")
            # Output: ['Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±', 'Ø§ÛŒÙ†ØªØ±Ù†Øª', 'Ø³ÛŒØ³ØªÙ…']

            print(f"Word mappings: {result.words}")
            # Output: {'Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±': 'Ø±Ø§ÛŒØ§Ù†Ù‡', 'Ø§ÛŒÙ†ØªØ±Ù†Øª': 'Ø§ÛŒÙ†ØªØ±Ù†Øª', 'Ø³ÛŒØ³ØªÙ…': 'Ø³Ø§Ù…Ø§Ù†Ù‡'}

            # Access statistics
            print(f"Total occurrences: {result.count}")
            # Output: 3

            print(f"Unique words: {result.unique_count}")
            # Output: 3

            print(f"Total words in text: {result.total_words}")
            # Output: 9

            print(f"Foreign percentage: {result.foreign_percentage:.2f}%")
            # Output: 33.33%

            # Get reports
            print("Detailed report:")
            print(result.to_text)
            # Output: ÙˆØ§Ú˜Ú¯Ø§Ù† Ø¨ÛŒÚ¯Ø§Ù†Ù‡ ÛŒØ§ÙØªâ€ŒØ´Ø¯Ù‡: Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ± (Ø±Ø§ÛŒØ§Ù†Ù‡), Ø§ÛŒÙ†ØªØ±Ù†Øª (Ø§ÛŒÙ†ØªØ±Ù†Øª), ...

            print("\nSummary:")
            print(result.to_summary_text)
            # Output: 3 ÙˆØ§Ú˜Ù‡ Ø¨ÛŒÚ¯Ø§Ù†Ù‡ Ø§Ø² 9 ÙˆØ§Ú˜Ù‡ (33.33Ùª)

        .. admonition:: Using DetectData effectively
            :class: tip

            * Use ``foreign_words`` for processing individual occurrences
            * Use ``words`` for unique word mappings
            * Use ``count`` vs ``unique_count`` to detect repetition
            * Use ``foreign_percentage`` for quality metrics
            * Use ``to_text`` for user-facing reports
            * Use ``to_summary_text`` for dashboards/statistics

    .. tab-item:: Ù¾Ø§Ø±Ø³ÛŒ

        Ø¸Ø±Ù Ù†ØªØ§ÛŒØ¬ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ùˆ Ø¢Ù…Ø§Ø±Ù‡Ø§. Ø§ÛŒÙ† Ø´ÛŒØ¡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¬Ø§Ù…Ø¹ÛŒ Ø¯Ø±Ø¨Ø§Ø±Ù‡Ù” ÙˆØ§Ú˜Ú¯Ø§Ù† Ø¨ÛŒÚ¯Ø§Ù†Ù‡ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒâ€ŒØ´Ø¯Ù‡ Ùˆ Ø¢Ù…Ø§Ø± Ù…ØªÙ† ÙØ±Ø§Ù‡Ù… Ù…ÛŒâ€ŒÚ©Ù†Ø¯.

        *ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§*

        .. py:attribute:: foreign_words
            :no-index:

            :type: list[str]

            ÙÙ‡Ø±Ø³Øª ÙˆØ§Ú˜Ú¯Ø§Ù† Ø¨ÛŒÚ¯Ø§Ù†Ù‡ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒâ€ŒØ´Ø¯Ù‡ (Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø¨Ø±Ø§ÛŒ Ø±Ø®Ø¯Ø§Ø¯Ù‡Ø§ÛŒ Ú†Ù†Ø¯Ú¯Ø§Ù†Ù‡ ØªÚ©Ø±Ø§Ø±ÛŒ Ø¨Ø§Ø´Ø¯)

        .. py:attribute:: words
            :no-index:

            :type: dict[str, str]

            Ù†Ú¯Ø§Ø´Øª ÙˆØ§Ú˜Ú¯Ø§Ù† Ø¨ÛŒÚ¯Ø§Ù†Ù‡ ÛŒÚ©ØªØ§ Ø¨Ù‡ Ø¨Ø±Ø§Ø¨Ø±Ù‡Ø§ÛŒ Ù¾Ø§Ø±Ø³ÛŒ Ø¢Ù†â€ŒÙ‡Ø§

        .. py:attribute:: text
            :type: str
            :no-index:

            Ù…ØªÙ† ÙˆØ±ÙˆØ¯ÛŒ Ø§ØµÙ„ÛŒ ÛŒØ§ Ù¾Ø±Ø¯Ø§Ø²Ø´â€ŒØ´Ø¯Ù‡

        .. py:attribute:: count
            :type: int
            :no-index:

            ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ø±Ø®Ø¯Ø§Ø¯Ù‡Ø§ÛŒ ÙˆØ§Ú˜Ú¯Ø§Ù† Ø¨ÛŒÚ¯Ø§Ù†Ù‡

        .. py:attribute:: unique_count
            :type: int
            :no-index:

            ØªØ¹Ø¯Ø§Ø¯ ÙˆØ§Ú˜Ú¯Ø§Ù† Ø¨ÛŒÚ¯Ø§Ù†Ù‡ ÛŒÚ©ØªØ§

        .. py:attribute:: total_words
            :type: int
            :no-index:

            ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ ÙˆØ§Ú˜Ú¯Ø§Ù† Ø¯Ø± Ù…ØªÙ†

        .. py:attribute:: foreign_percentage
            :type: float
            :no-index:

            Ø¯Ø±ØµØ¯ ÙˆØ§Ú˜Ú¯Ø§Ù† Ø¨ÛŒÚ¯Ø§Ù†Ù‡ Ø¯Ø± Ù…ØªÙ†

        .. py:attribute:: to_text
            :type: str
            :no-index:

            Ú¯Ø²Ø§Ø±Ø´ Ù…ØªÙ†ÛŒ ØªÙØµÛŒÙ„ÛŒ Ø¨Ù‡ Ù¾Ø§Ø±Ø³ÛŒ

        .. py:attribute:: to_summary_text
            :type: str
            :no-index:

            Ú¯Ø²Ø§Ø±Ø´ Ø®Ù„Ø§ØµÙ‡ Ø¨Ù‡ Ù¾Ø§Ø±Ø³ÛŒ

        *Ù†Ù…ÙˆÙ†Ù‡ Ú©Ø§Ø±Ø¨Ø±Ø¯*

        .. code-block:: python

            from pasban.detector import WordDetector

            detector = WordDetector()
            text = "Ø§ÛŒÙ† Ù…ØªÙ† Ø´Ø§Ù…Ù„ Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ± Ùˆ Ø§ÛŒÙ†ØªØ±Ù†Øª Ùˆ Ø³ÛŒØ³ØªÙ… Ø§Ø³Øª."
            result = detector.detect(text)

            # Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù¾Ø§ÛŒÙ‡
            print(f"ÙÙ‡Ø±Ø³Øª ÙˆØ§Ú˜Ú¯Ø§Ù† Ø¨ÛŒÚ¯Ø§Ù†Ù‡: {result.foreign_words}")
            # Ø®Ø±ÙˆØ¬ÛŒ: ['Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±', 'Ø§ÛŒÙ†ØªØ±Ù†Øª', 'Ø³ÛŒØ³ØªÙ…']

            print(f"Ù†Ú¯Ø§Ø´Øª ÙˆØ§Ú˜Ú¯Ø§Ù†: {result.words}")
            # Ø®Ø±ÙˆØ¬ÛŒ: {'Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±': 'Ø±Ø§ÛŒØ§Ù†Ù‡', 'Ø§ÛŒÙ†ØªØ±Ù†Øª': 'Ø§ÛŒÙ†ØªØ±Ù†Øª', 'Ø³ÛŒØ³ØªÙ…': 'Ø³Ø§Ù…Ø§Ù†Ù‡'}

            # Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ Ø¢Ù…Ø§Ø±Ù‡Ø§
            print(f"ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ø±Ø®Ø¯Ø§Ø¯Ù‡Ø§: {result.count}")
            # Ø®Ø±ÙˆØ¬ÛŒ: 3

            print(f"ÙˆØ§Ú˜Ú¯Ø§Ù† ÛŒÚ©ØªØ§: {result.unique_count}")
            # Ø®Ø±ÙˆØ¬ÛŒ: 3

            print(f"Ú©Ù„ ÙˆØ§Ú˜Ú¯Ø§Ù† Ù…ØªÙ†: {result.total_words}")
            # Ø®Ø±ÙˆØ¬ÛŒ: 9

            print(f"Ø¯Ø±ØµØ¯ ÙˆØ§Ú˜Ú¯Ø§Ù† Ø¨ÛŒÚ¯Ø§Ù†Ù‡: {result.foreign_percentage:.2f}%")
            # Ø®Ø±ÙˆØ¬ÛŒ: 33.33%

            # Ø¯Ø±ÛŒØ§ÙØª Ú¯Ø²Ø§Ø±Ø´â€ŒÙ‡Ø§
            print("Ú¯Ø²Ø§Ø±Ø´ ØªÙØµÛŒÙ„ÛŒ:")
            print(result.to_text)
            # Ø®Ø±ÙˆØ¬ÛŒ: ÙˆØ§Ú˜Ú¯Ø§Ù† Ø¨ÛŒÚ¯Ø§Ù†Ù‡ ÛŒØ§ÙØªâ€ŒØ´Ø¯Ù‡: Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ± (Ø±Ø§ÛŒØ§Ù†Ù‡), Ø§ÛŒÙ†ØªØ±Ù†Øª (Ø§ÛŒÙ†ØªØ±Ù†Øª), ...

            print("\nØ®Ù„Ø§ØµÙ‡:")
            print(result.to_summary_text)
            # Ø®Ø±ÙˆØ¬ÛŒ: 3 ÙˆØ§Ú˜Ù‡ Ø¨ÛŒÚ¯Ø§Ù†Ù‡ Ø§Ø² 9 ÙˆØ§Ú˜Ù‡ (33.33Ùª)

        .. admonition:: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…Ø¤Ø«Ø± Ø§Ø² DetectData
            :class: tip

            * Ø§Ø² ``foreign_words`` Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø±Ø®Ø¯Ø§Ø¯Ù‡Ø§ÛŒ Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯
            * Ø§Ø² ``words`` Ø¨Ø±Ø§ÛŒ Ù†Ú¯Ø§Ø´Øª ÙˆØ§Ú˜Ú¯Ø§Ù† ÛŒÚ©ØªØ§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯
            * Ø§Ø² ``count`` Ø¯Ø± Ù…Ù‚Ø§Ø¨Ù„ ``unique_count`` Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ ØªÚ©Ø±Ø§Ø± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯
            * Ø§Ø² ``foreign_percentage`` Ø¨Ø±Ø§ÛŒ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ú©ÛŒÙÛŒØª Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯
            * Ø§Ø² ``to_text`` Ø¨Ø±Ø§ÛŒ Ú¯Ø²Ø§Ø±Ø´â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø±ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯
            * Ø§Ø² ``to_summary_text`` Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø´Ø¨ÙˆØ±Ø¯Ù‡Ø§/Ø¢Ù…Ø§Ø±Ù‡Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯


AhoCorasickAutomaton
--------------------

.. tab-set::

    .. tab-item:: English

        Multi-pattern string matching engine using the Aho-Corasick algorithm. This is the core algorithm used by ``WordDetector`` for fast detection of multiple patterns simultaneously.

        *Methods*

        .. py:method:: add_word(word: str) -> None
            :no-index:

            Add a word to the automaton trie.

            :param word: Word to add

        .. py:method:: build_failure_links() -> None

            Build failure links for fast matching. Must be called after adding all words and before searching.

        .. py:method:: search(text: str) -> List[Tuple[str, int, int]]

            Find all pattern matches in the text.

            :param text: Text to search in
            :return: List of tuples (matched_word, start_position, end_position)

        *Example Usage*

        .. code-block:: python

            from pasban.core import AhoCorasickAutomaton

            # Create automaton
            ac = AhoCorasickAutomaton()

            # Add patterns
            ac.add_word("Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±")
            ac.add_word("Ø§ÛŒÙ†ØªØ±Ù†Øª")
            ac.add_word("Ø³ÛŒØ³ØªÙ…")

            # Build failure links (required before searching)
            ac.build_failure_links()

            # Search for patterns
            text = "Ø§ÛŒÙ† Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ± Ø¨Ù‡ Ø§ÛŒÙ†ØªØ±Ù†Øª Ù…ØªØµÙ„ Ø§Ø³Øª Ùˆ Ø³ÛŒØ³ØªÙ… Ø¹Ø§Ù…Ù„ Ø¯Ø§Ø±Ø¯."
            matches = ac.search(text)

            # Process results
            for word, start, end in matches:
                print(f"Found '{word}' at position {start}-{end}")
            # Output:
            # Found 'Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±' at position 4-12
            # Found 'Ø§ÛŒÙ†ØªØ±Ù†Øª' at position 16-23
            # Found 'Ø³ÛŒØ³ØªÙ…' at position 32-37

        *Advanced Usage*

        .. code-block:: python

            from pasban.core import AhoCorasickAutomaton

            # Build automaton from word list
            words = ["Ø±Ø§ÛŒØ§Ù†Ù‡", "Ø§ÛŒÙ†ØªØ±Ù†Øª", "Ø´Ø¨Ú©Ù‡", "Ø³Ø§Ù…Ø§Ù†Ù‡"]
            ac = AhoCorasickAutomaton()

            for word in words:
                ac.add_word(word)

            ac.build_failure_links()

            # Search and extract context
            text = "Ø±Ø§ÛŒØ§Ù†Ù‡ Ø¨Ù‡ Ø´Ø¨Ú©Ù‡ Ø§ÛŒÙ†ØªØ±Ù†Øª Ù…ØªØµÙ„ Ø§Ø³Øª."
            matches = ac.search(text)

            for word, start, end in matches:
                # Extract context (10 chars before and after)
                context_start = max(0, start - 10)
                context_end = min(len(text), end + 10)
                context = text[context_start:context_end]
                print(f"{word}: ...{context}...")

        .. admonition:: Performance characteristics
            :class: note

            * Time complexity: O(n + m + z) where n=text length, m=total pattern length, z=matches
            * Space complexity: O(m) for the trie structure
            * Initialization: O(m) to build the trie and failure links
            * Optimal for: Multiple patterns, large texts, repeated searches

    .. tab-item:: Ù¾Ø§Ø±Ø³ÛŒ

        Ù…ÙˆØªÙˆØ± Ø¬Ø³ØªØ¬ÙˆÛŒ Ú†Ù†Ø¯Ø§Ù„Ú¯ÙˆÛŒÛŒ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ø¢Ù‡Ùˆ-Ú©ÙØ±Ø§Ø³ÛŒÚ©. Ø§ÛŒÙ† Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ù‡Ø³ØªÙ‡Ù” Ø§ØµÙ„ÛŒ Ø§Ø³Øª Ú©Ù‡ ``WordDetector`` Ø¨Ø±Ø§ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø³Ø±ÛŒØ¹ Ú†Ù†Ø¯ÛŒÙ† Ø§Ù„Ú¯Ùˆ Ø¨Ù‡â€ŒØ·ÙˆØ± Ù‡Ù…Ø²Ù…Ø§Ù† Ø§Ø² Ø¢Ù† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.

        *Ù…ØªØ¯Ù‡Ø§*

        .. py:method:: add_word(word: str) -> None
            :no-index:

            Ø§ÙØ²ÙˆØ¯Ù† ÙˆØ§Ú˜Ù‡ Ø¨Ù‡ Ø¯Ø±Ø®Øª Ø®ÙˆØ¯Ú©Ø§Ø±.

            :param word: ÙˆØ§Ú˜Ù‡ Ø¨Ø±Ø§ÛŒ Ø§ÙØ²ÙˆØ¯Ù†

        .. py:method:: build_failure_links() -> None
            :no-index:

            Ø³Ø§Ø®Øª Ù¾ÛŒÙˆÙ†Ø¯Ù‡Ø§ÛŒ Ø´Ú©Ø³Øª Ø¨Ø±Ø§ÛŒ ØªØ·Ø¨ÛŒÙ‚ Ø³Ø±ÛŒØ¹. Ø¨Ø§ÛŒØ¯ Ù¾Ø³ Ø§Ø² Ø§ÙØ²ÙˆØ¯Ù† Ù‡Ù…Ù‡Ù” ÙˆØ§Ú˜Ú¯Ø§Ù† Ùˆ Ù¾ÛŒØ´ Ø§Ø² Ø¬Ø³ØªØ¬Ùˆ ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ø´ÙˆØ¯.

        .. py:method:: search(text: str) -> List[Tuple[str, int, int]]
            :no-index:

            ÛŒØ§ÙØªÙ† Ù‡Ù…Ù‡Ù” ØªØ·Ø¨ÛŒÙ‚â€ŒÙ‡Ø§ÛŒ Ø§Ù„Ú¯Ùˆ Ø¯Ø± Ù…ØªÙ†.

            :param text: Ù…ØªÙ† Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬Ùˆ
            :return: ÙÙ‡Ø±Ø³Øª ØªØ§Ù¾Ù„â€ŒÙ‡Ø§ (ÙˆØ§Ú˜Ù‡_ØªØ·Ø¨ÛŒÙ‚â€ŒÛŒØ§ÙØªÙ‡ØŒ Ù…ÙˆÙ‚Ø¹ÛŒØª_Ø¢ØºØ§Ø²ØŒ Ù…ÙˆÙ‚Ø¹ÛŒØª_Ù¾Ø§ÛŒØ§Ù†)

        *Ù†Ù…ÙˆÙ†Ù‡ Ú©Ø§Ø±Ø¨Ø±Ø¯*

        .. code-block:: python

            from pasban.core import AhoCorasickAutomaton

            # Ø³Ø§Ø®Øª Ø®ÙˆØ¯Ú©Ø§Ø±
            ac = AhoCorasickAutomaton()

            # Ø§ÙØ²ÙˆØ¯Ù† Ø§Ù„Ú¯ÙˆÙ‡Ø§
            ac.add_word("Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±")
            ac.add_word("Ø§ÛŒÙ†ØªØ±Ù†Øª")
            ac.add_word("Ø³ÛŒØ³ØªÙ…")

            # Ø³Ø§Ø®Øª Ù¾ÛŒÙˆÙ†Ø¯Ù‡Ø§ÛŒ Ø´Ú©Ø³Øª (Ù„Ø§Ø²Ù… Ù¾ÛŒØ´ Ø§Ø² Ø¬Ø³ØªØ¬Ùˆ)
            ac.build_failure_links()

            # Ø¬Ø³ØªØ¬ÙˆÛŒ Ø§Ù„Ú¯ÙˆÙ‡Ø§
            text = "Ø§ÛŒÙ† Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ± Ø¨Ù‡ Ø§ÛŒÙ†ØªØ±Ù†Øª Ù…ØªØµÙ„ Ø§Ø³Øª Ùˆ Ø³ÛŒØ³ØªÙ… Ø¹Ø§Ù…Ù„ Ø¯Ø§Ø±Ø¯."
            matches = ac.search(text)

            # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù†ØªØ§ÛŒØ¬
            for word, start, end in matches:
                print(f"'{word}' Ø¯Ø± Ù…ÙˆÙ‚Ø¹ÛŒØª {start}-{end} ÛŒØ§ÙØª Ø´Ø¯")
            # Ø®Ø±ÙˆØ¬ÛŒ:
            # 'Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±' Ø¯Ø± Ù…ÙˆÙ‚Ø¹ÛŒØª 4-12 ÛŒØ§ÙØª Ø´Ø¯
            # 'Ø§ÛŒÙ†ØªØ±Ù†Øª' Ø¯Ø± Ù…ÙˆÙ‚Ø¹ÛŒØª 16-23 ÛŒØ§ÙØª Ø´Ø¯
            # 'Ø³ÛŒØ³ØªÙ…' Ø¯Ø± Ù…ÙˆÙ‚Ø¹ÛŒØª 32-37 ÛŒØ§ÙØª Ø´Ø¯

        *Ú©Ø§Ø±Ø¨Ø±Ø¯ Ù¾ÛŒØ´Ø±ÙØªÙ‡*

        .. code-block:: python

            from pasban.core import AhoCorasickAutomaton

            # Ø³Ø§Ø®Øª Ø®ÙˆØ¯Ú©Ø§Ø± Ø§Ø² ÙÙ‡Ø±Ø³Øª ÙˆØ§Ú˜Ú¯Ø§Ù†
            words = ["Ø±Ø§ÛŒØ§Ù†Ù‡", "Ø§ÛŒÙ†ØªØ±Ù†Øª", "Ø´Ø¨Ú©Ù‡", "Ø³Ø§Ù…Ø§Ù†Ù‡"]
            ac = AhoCorasickAutomaton()

            for word in words:
                ac.add_word(word)

            ac.build_failure_links()

            # Ø¬Ø³ØªØ¬Ùˆ Ùˆ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø²Ù…ÛŒÙ†Ù‡
            text = "Ø±Ø§ÛŒØ§Ù†Ù‡ Ø¨Ù‡ Ø´Ø¨Ú©Ù‡ Ø§ÛŒÙ†ØªØ±Ù†Øª Ù…ØªØµÙ„ Ø§Ø³Øª."
            matches = ac.search(text)

            for word, start, end in matches:
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø²Ù…ÛŒÙ†Ù‡ (Û±Û° Ù†ÙˆÛŒØ³Ù‡ Ù¾ÛŒØ´ Ùˆ Ù¾Ø³)
                context_start = max(0, start - 10)
                context_end = min(len(text), end + 10)
                context = text[context_start:context_end]
                print(f"{word}: ...{context}...")

        .. admonition:: ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ú©Ø§Ø±Ø§ÛŒÛŒ
            :class: note

            * Ù¾ÛŒÚ†ÛŒØ¯Ú¯ÛŒ Ø²Ù…Ø§Ù†ÛŒ: O(n + m + z) Ú©Ù‡ n=Ø·ÙˆÙ„ Ù…ØªÙ†ØŒ m=Ù…Ø¬Ù…ÙˆØ¹ Ø·ÙˆÙ„ Ø§Ù„Ú¯ÙˆÙ‡Ø§ØŒ z=ØªØ·Ø¨ÛŒÙ‚â€ŒÙ‡Ø§
            * Ù¾ÛŒÚ†ÛŒØ¯Ú¯ÛŒ ÙØ¶Ø§ÛŒÛŒ: O(m) Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø®ØªØ§Ø± Ø¯Ø±Ø®Øª
            * Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ: O(m) Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø®Øª Ø¯Ø±Ø®Øª Ùˆ Ù¾ÛŒÙˆÙ†Ø¯Ù‡Ø§ÛŒ Ø´Ú©Ø³Øª
            * Ø¨Ù‡ÛŒÙ†Ù‡ Ø¨Ø±Ø§ÛŒ: Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ú†Ù†Ø¯Ú¯Ø§Ù†Ù‡ØŒ Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ Ø¨Ø²Ø±Ú¯ØŒ Ø¬Ø³ØªØ¬ÙˆÙ‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ


Best Practices | Ù†Ú©Ø§Øª Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ
=============================

.. tab-set::

    .. tab-item:: English

        *General Guidelines*

        .. grid:: 2
            :gutter: 3

            .. grid-item-card:: âœ… Always Do
                :class-card: sd-border-success

                * Enable normalization for consistent results
                * Enable contextual cleaning for literary texts
                * Use ``WordDetector`` for production environments
                * Reuse detector instances (avoid re-initialization)
                * Check ``foreign_percentage`` for quality metrics
                * Handle edge cases (empty text, pure Persian text)

            .. grid-item-card:: âš ï¸ Be Careful
                :class-card: sd-border-warning

                * Don't disable normalization unless necessary
                * Don't disable contextual cleaning for formal texts
                * Be aware of initialization cost (~85ms)
                * Consider caching detector instances
                * Validate input text before processing
                * Handle encoding issues properly

        *Performance Optimization*

        .. code-block:: python

            from pasban.detector import WordDetector

            # âœ… Good: Initialize once, use many times
            detector = WordDetector()

            texts = ["Ù…ØªÙ† Ø§ÙˆÙ„", "Ù…ØªÙ† Ø¯ÙˆÙ…", "Ù…ØªÙ† Ø³ÙˆÙ…"]
            for text in texts:
                result = detector.detect(text)
                print(result.count)

            # âŒ Bad: Re-initialize for each text
            for text in texts:
                detector = WordDetector()  # Wasteful!
                result = detector.detect(text)

        .. code-block:: python

            # âœ… Good: Disable options only when needed
            # For maximum speed on trusted, pre-normalized text
            result = detector.detect(text, normalize=False, contextual=False)

            # âœ… Good: Use appropriate engine
            # Large corpus processing
            detector = WordDetector()  # Fast

            # Small, critical accuracy scenarios
            detector = WordDetectorRegex()  # Accurate

        *Database Management*

        .. code-block:: python

            from pasban.db import WordRepo

            repo = WordRepo()

            # âœ… Good: Batch operations
            new_words = {
                "ÙˆÛŒØ¯Ø¦Ùˆ": "ÙˆÛŒØ¯ÛŒÙˆ",
                "Ú©Ù„ÛŒÙ¾": "Ø¨Ø±ÛŒØ¯Ù‡",
                "ÙØ§ÛŒÙ„": "Ù¾Ø±ÙˆÙ†Ø¯Ù‡"
            }

            for foreign, persian in new_words.items():
                repo.add_word(foreign, persian)

            # After updates, reload detector
            detector.reload()

            # âœ… Good: Export and backup
            all_words = repo.get_all_words()
            # Save to file for backup

        *Error Handling*

        .. code-block:: python

            from pasban.detector import WordDetector
            from pasban.db import WordRepo

            def safe_detect(text: str) -> DetectData:
                """Safely detect foreign words with error handling."""
                try:
                    if not text or not text.strip():
                        # Handle empty text
                        return None

                    detector = WordDetector()
                    result = detector.detect(text)
                    return result

                except Exception as e:
                    print(f"Error detecting words: {e}")
                    return None

            # Usage
            result = safe_detect("Ù…ØªÙ† Ù†Ù…ÙˆÙ†Ù‡")
            if result:
                print(f"Found {result.count} foreign words")

        *Quality Assurance*

        .. code-block:: python

            def assess_text_quality(text: str) -> dict:
                """Assess Persian text quality based on foreign word percentage."""
                detector = WordDetector()
                result = detector.detect(text)

                quality = {
                    "foreign_count": result.count,
                    "foreign_percentage": result.foreign_percentage,
                    "total_words": result.total_words,
                    "status": "unknown"
                }

                # Define quality levels
                if result.foreign_percentage < 5:
                    quality["status"] = "excellent"
                elif result.foreign_percentage < 15:
                    quality["status"] = "good"
                elif result.foreign_percentage < 30:
                    quality["status"] = "acceptable"
                else:
                    quality["status"] = "needs_improvement"

                return quality

            # Usage
            text = "Ø§ÛŒÙ† Ù…ØªÙ† Ø´Ø§Ù…Ù„ Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ± Ùˆ Ø§ÛŒÙ†ØªØ±Ù†Øª Ø§Ø³Øª."
            quality = assess_text_quality(text)
            print(f"Text quality: {quality['status']}")
            print(f"Foreign words: {quality['foreign_percentage']:.1f}%")

    .. tab-item:: Ù¾Ø§Ø±Ø³ÛŒ

        *Ø±Ø§Ù‡Ù†Ù…Ø§Ù‡Ø§ÛŒ Ú©Ù„ÛŒ*

        .. grid:: 2
            :gutter: 3

            .. grid-item-card:: âœ… Ù‡Ù…ÙˆØ§Ø±Ù‡ Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡ÛŒØ¯
                :class-card: sd-border-success

                * Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ù†ØªØ§ÛŒØ¬ ÛŒÚ©Ø³Ø§Ù† ÙØ¹Ø§Ù„ Ú©Ù†ÛŒØ¯
                * Ù¾Ø§Ù„Ø§ÛŒØ´ Ø²Ù…ÛŒÙ†Ù‡â€ŒØ§ÛŒ Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ Ø§Ø¯Ø¨ÛŒ ÙØ¹Ø§Ù„ Ú©Ù†ÛŒØ¯
                * Ø§Ø² ``WordDetector`` Ø¯Ø± Ù…Ø­ÛŒØ· ØªÙˆÙ„ÛŒØ¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯
                * Ø§Ø² Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø´Ù†Ø§Ø³Ø§Ú¯Ø± Ù…Ø¬Ø¯Ø¯Ø§Ù‹ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯ (Ø§Ø² Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ø¨Ù¾Ø±Ù‡ÛŒØ²ÛŒØ¯)
                * ``foreign_percentage`` Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ø³Ù†Ø¬Ø´ Ú©ÛŒÙÛŒØª Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯
                * Ù…ÙˆØ§Ø±Ø¯ Ø§Ø³ØªØ«Ù†Ø§ÛŒÛŒ Ø±Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ú©Ù†ÛŒØ¯ (Ù…ØªÙ† Ø®Ø§Ù„ÛŒØŒ Ù…ØªÙ† Ù¾Ø§Ø±Ø³ÛŒ Ø³Ø±Ù‡)

            .. grid-item-card:: âš ï¸ Ø§Ø­ØªÛŒØ§Ø· Ú©Ù†ÛŒØ¯
                :class-card: sd-border-warning

                * Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø±Ø§ Ø¨Ø¯ÙˆÙ† Ù†ÛŒØ§Ø² ØºÛŒØ±ÙØ¹Ø§Ù„ Ù†Ú©Ù†ÛŒØ¯
                * Ù¾Ø§Ù„Ø§ÛŒØ´ Ø²Ù…ÛŒÙ†Ù‡â€ŒØ§ÛŒ Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ Ø±Ø³Ù…ÛŒ ØºÛŒØ±ÙØ¹Ø§Ù„ Ù†Ú©Ù†ÛŒØ¯
                * Ø§Ø² Ù‡Ø²ÛŒÙ†Ù‡ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ (~Û¸Ûµ Ù…ÛŒÙ„ÛŒâ€ŒØ«Ø§Ù†ÛŒÙ‡) Ø¢Ú¯Ø§Ù‡ Ø¨Ø§Ø´ÛŒØ¯
                * Ø­Ø§ÙØ¸Ù‡â€ŒÙ†Ù‡Ø§Ù†ÛŒ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø´Ù†Ø§Ø³Ø§Ú¯Ø± Ø±Ø§ Ø¯Ø± Ù†Ø¸Ø± Ø¨Ú¯ÛŒØ±ÛŒØ¯
                * Ù…ØªÙ† ÙˆØ±ÙˆØ¯ÛŒ Ø±Ø§ Ù¾ÛŒØ´ Ø§Ø² Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ú©Ù†ÛŒØ¯
                * Ù…Ø³Ø§Ø¦Ù„ Ú©Ø¯Ú¯Ø°Ø§Ø±ÛŒ Ø±Ø§ Ø¨Ù‡â€ŒØ¯Ø±Ø³ØªÛŒ Ù…Ø¯ÛŒØ±ÛŒØª Ú©Ù†ÛŒØ¯

        *Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ø§Ø±Ø§ÛŒÛŒ*

        .. code-block:: python

            from pasban.detector import WordDetector

            # âœ… Ø®ÙˆØ¨: ÛŒÚ©â€ŒØ¨Ø§Ø± Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒØŒ Ú†Ù†Ø¯ÛŒÙ†â€ŒØ¨Ø§Ø± Ø§Ø³ØªÙØ§Ø¯Ù‡
            detector = WordDetector()

            texts = ["Ù…ØªÙ† Ø§ÙˆÙ„", "Ù…ØªÙ† Ø¯ÙˆÙ…", "Ù…ØªÙ† Ø³ÙˆÙ…"]
            for text in texts:
                result = detector.detect(text)
                print(result.count)

            # âŒ Ø¨Ø¯: Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù…ØªÙ†
            for text in texts:
                detector = WordDetector()  # Ø§ØªÙ„Ø§Ù Ù…Ù†Ø§Ø¨Ø¹!
                result = detector.detect(text)

        .. code-block:: python

            # âœ… Ø®ÙˆØ¨: Ú¯Ø²ÛŒÙ†Ù‡â€ŒÙ‡Ø§ Ø±Ø§ ÙÙ‚Ø· Ø¯Ø± ØµÙˆØ±Øª Ù†ÛŒØ§Ø² ØºÛŒØ±ÙØ¹Ø§Ù„ Ú©Ù†ÛŒØ¯
            # Ø¨Ø±Ø§ÛŒ Ø³Ø±Ø¹Øª Ø¨ÛŒØ´ÛŒÙ†Ù‡ Ø¨Ø± Ø±ÙˆÛŒ Ù…ØªÙ† Ù†Ø±Ù…Ø§Ù„â€ŒØ´Ø¯Ù‡ Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªÙ…Ø§Ø¯
            result = detector.detect(text, normalize=False, contextual=False)

            # âœ… Ø®ÙˆØ¨: Ø§Ø² Ù…ÙˆØªÙˆØ± Ù…Ù†Ø§Ø³Ø¨ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯
            # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ Ø¨Ø²Ø±Ú¯
            detector = WordDetector()  # Ø³Ø±ÛŒØ¹

            # Ø³Ù†Ø§Ø±ÛŒÙˆÙ‡Ø§ÛŒ Ú©ÙˆÚ†Ú© Ø¨Ø§ Ù…ÙˆØ´Ú©Ø§ÙÛŒ Ø­ÛŒØ§ØªÛŒ
            detector = WordDetectorRegex()  # Ø¯Ù‚ÛŒÙ‚

        *Ù…Ø¯ÛŒØ±ÛŒØª Ù¾Ø§ÛŒÚ¯Ø§Ù‡â€ŒØ¯Ø§Ø¯Ù‡*

        .. code-block:: python

            from pasban.db import WordRepo

            repo = WordRepo()

            # âœ… Ø®ÙˆØ¨: Ø¹Ù…Ù„ÛŒØ§Øª Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ
            new_words = {
                "ÙˆÛŒØ¯Ø¦Ùˆ": "ÙˆÛŒØ¯ÛŒÙˆ",
                "Ú©Ù„ÛŒÙ¾": "Ø¨Ø±ÛŒØ¯Ù‡",
                "ÙØ§ÛŒÙ„": "Ù¾Ø±ÙˆÙ†Ø¯Ù‡"
            }

            for foreign, persian in new_words.items():
                repo.add_word(foreign, persian)

            # Ù¾Ø³ Ø§Ø² Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒØŒ Ø´Ù†Ø§Ø³Ø§Ú¯Ø± Ø±Ø§ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ú©Ù†ÛŒØ¯
            detector.reload()

            # âœ… Ø®ÙˆØ¨: Ø¨Ø±ÙˆÙ†â€ŒØ¨Ø±ÛŒ Ùˆ Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ
            all_words = repo.get_all_words()
            # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ù¾Ø±ÙˆÙ†Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ù¾Ø´ØªÛŒØ¨Ø§Ù†

        *Ù…Ø¯ÛŒØ±ÛŒØª Ø®Ø·Ø§*

        .. code-block:: python

            from pasban.detector import WordDetector
            from pasban.db import WordRepo

            def safe_detect(text: str) -> DetectData:
                """Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø§ÛŒÙ…Ù† ÙˆØ§Ú˜Ú¯Ø§Ù† Ø¨ÛŒÚ¯Ø§Ù†Ù‡ Ø¨Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ø®Ø·Ø§."""
                try:
                    if not text or not text.strip():
                        # Ù…Ø¯ÛŒØ±ÛŒØª Ù…ØªÙ† Ø®Ø§Ù„ÛŒ
                        return None

                    detector = WordDetector()
                    result = detector.detect(text)
                    return result

                except Exception as e:
                    print(f"Ø®Ø·Ø§ Ø¯Ø± Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ ÙˆØ§Ú˜Ú¯Ø§Ù†: {e}")
                    return None

            # Ø§Ø³ØªÙØ§Ø¯Ù‡
            result = safe_detect("Ù…ØªÙ† Ù†Ù…ÙˆÙ†Ù‡")
            if result:
                print(f"{result.count} ÙˆØ§Ú˜Ù‡ Ø¨ÛŒÚ¯Ø§Ù†Ù‡ ÛŒØ§ÙØª Ø´Ø¯")

        *ØªØ¶Ù…ÛŒÙ† Ú©ÛŒÙÛŒØª*

        .. code-block:: python

            def assess_text_quality(text: str) -> dict:
                """Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ú©ÛŒÙÛŒØª Ù…ØªÙ† Ù¾Ø§Ø±Ø³ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ø±ØµØ¯ ÙˆØ§Ú˜Ú¯Ø§Ù† Ø¨ÛŒÚ¯Ø§Ù†Ù‡."""
                detector = WordDetector()
                result = detector.detect(text)

                quality = {
                    "foreign_count": result.count,
                    "foreign_percentage": result.foreign_percentage,
                    "total_words": result.total_words,
                    "status": "Ù†Ø§Ù…Ø´Ø®Øµ"
                }

                # ØªØ¹Ø±ÛŒÙ Ø³Ø·ÙˆØ­ Ú©ÛŒÙÛŒØª
                if result.foreign_percentage < 5:
                    quality["status"] = "Ø¹Ø§Ù„ÛŒ"
                elif result.foreign_percentage < 15:
                    quality["status"] = "Ø®ÙˆØ¨"
                elif result.foreign_percentage < 30:
                    quality["status"] = "Ù‚Ø§Ø¨Ù„â€ŒÙ‚Ø¨ÙˆÙ„"
                else:
                    quality["status"] = "Ù†ÛŒØ§Ø²_Ø¨Ù‡_Ø¨Ù‡Ø¨ÙˆØ¯"

                return quality

            # Ø§Ø³ØªÙØ§Ø¯Ù‡
            text = "Ø§ÛŒÙ† Ù…ØªÙ† Ø´Ø§Ù…Ù„ Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ± Ùˆ Ø§ÛŒÙ†ØªØ±Ù†Øª Ø§Ø³Øª."
            quality = assess_text_quality(text)
            print(f"Ú©ÛŒÙÛŒØª Ù…ØªÙ†: {quality['status']}")
            print(f"ÙˆØ§Ú˜Ú¯Ø§Ù† Ø¨ÛŒÚ¯Ø§Ù†Ù‡: {quality['foreign_percentage']:.1f}%")


Use Cases | Ù…ÙˆØ§Ø±Ø¯ Ú©Ø§Ø±Ø¨Ø±Ø¯
-------------------------

.. tab-set::

    .. tab-item:: English

        *Content Quality Control*

        .. code-block:: python

            from pasban.detector import WordDetector

            def validate_persian_content(content: str, threshold: float = 20.0) -> tuple[bool, str]:
                """Validate that content meets Persian purity standards."""
                detector = WordDetector()
                result = detector.detect(content)

                if result.foreign_percentage > threshold:
                    message = f"Content contains {result.foreign_percentage:.1f}% foreign words (threshold: {threshold}%)"
                    return False, message

                return True, "Content meets purity standards"

            # Usage in CMS
            article_text = "Ø§ÛŒÙ† Ù…Ù‚Ø§Ù„Ù‡ Ø¯Ø±Ø¨Ø§Ø±Ù‡Ù” Ø±Ø§ÛŒØ§Ù†Ù‡ Ø§Ø³Øª..."
            is_valid, message = validate_persian_content(article_text)
            if not is_valid:
                print(f"Rejected: {message}")

        *Text Processing Pipeline*

        .. code-block:: python

            from pasban.detector import WordDetector
            from pasban.normalizer.text_normalizer import WordNormalizer

            class PersianTextProcessor:
                def __init__(self):
                    self.detector = WordDetector()

                def process(self, text: str) -> dict:
                    """Complete text processing pipeline."""
                    # Step 1: Normalize
                    normalized = WordNormalizer.normalize_text(text)

                    # Step 2: Detect foreign words
                    result = self.detector.detect(normalized)

                    # Step 3: Generate report
                    report = {
                        "original_text": text,
                        "normalized_text": normalized,
                        "foreign_words": result.words,
                        "statistics": {
                            "total_words": result.total_words,
                            "foreign_count": result.count,
                            "foreign_percentage": result.foreign_percentage
                        },
                        "quality": self._assess_quality(result.foreign_percentage)
                    }

                    return report

                def _assess_quality(self, percentage: float) -> str:
                    if percentage < 5:
                        return "excellent"
                    elif percentage < 15:
                        return "good"
                    elif percentage < 30:
                        return "acceptable"
                    return "poor"

            # Usage
            processor = PersianTextProcessor()
            report = processor.process("Ø§ÛŒÙ† Ù…ØªÙ† Ø´Ø§Ù…Ù„ Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ± Ø§Ø³Øª.")
            print(f"Quality: {report['quality']}")
            print(f"Foreign words: {report['foreign_words']}")

        *Educational Applications*

        .. code-block:: python

            from pasban.detector import WordDetector

            def create_learning_material(text: str) -> str:
                """Create educational material showing foreign word alternatives."""
                detector = WordDetector()
                result = detector.detect(text)

                if result.count == 0:
                    return "âœ… Ø§ÛŒÙ† Ù…ØªÙ† Ø³Ø±Ø§Ø³Ø± Ù¾Ø§Ø±Ø³ÛŒ Ø§Ø³Øª!"

                output = ["ğŸ“š ÙˆØ§Ú˜Ú¯Ø§Ù† Ø¨ÛŒÚ¯Ø§Ù†Ù‡ Ùˆ Ø¨Ø±Ø§Ø¨Ø±Ù‡Ø§ÛŒ Ù¾Ø§Ø±Ø³ÛŒ Ø¢Ù†â€ŒÙ‡Ø§:\n"]

                for foreign, persian in result.words.items():
                    if foreign != persian:
                        output.append(f"âŒ {foreign} â†’ âœ… {persian}")
                    else:
                        output.append(f"âš ï¸ {foreign} (Ø¨Ø±Ø§Ø¨Ø± Ù¾Ø§Ø±Ø³ÛŒ Ù†Ø¯Ø§Ø±Ø¯)")

                output.append(f"\nğŸ“Š Ø¢Ù…Ø§Ø±: {result.count} ÙˆØ§Ú˜Ù‡ Ø¨ÛŒÚ¯Ø§Ù†Ù‡ Ø§Ø² {result.total_words} ÙˆØ§Ú˜Ù‡ ({result.foreign_percentage:.1f}%)")

                return "\n".join(output)

            # Usage
            student_text = "Ù…Ù† Ø¨Ø§ Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ± Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ù… Ùˆ Ø§Ø² Ø§ÛŒÙ†ØªØ±Ù†Øª Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ù…."
            learning_material = create_learning_material(student_text)
            print(learning_material)

        *Batch Document Processing*

        .. code-block:: python

            from pasban.detector import WordDetector
            from pathlib import Path
            import json

            def process_documents(input_dir: str, output_file: str):
                """Process multiple documents and generate comprehensive report."""
                detector = WordDetector()
                results = []

                for file_path in Path(input_dir).glob("*.txt"):
                    with open(file_path, 'r', encoding='utf-8') as f:
                        text = f.read()

                    result = detector.detect(text)

                    results.append({
                        "filename": file_path.name,
                        "total_words": result.total_words,
                        "foreign_count": result.count,
                        "foreign_percentage": result.foreign_percentage,
                        "foreign_words": list(result.words.keys())
                    })

                # Save report
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(results, f, ensure_ascii=False, indent=2)

                return results

            # Usage
            results = process_documents("./documents", "./report.json")
            print(f"Processed {len(results)} documents")

    .. tab-item:: Ù¾Ø§Ø±Ø³ÛŒ

        *Ú©Ù†ØªØ±Ù„ Ú©ÛŒÙÛŒØª Ù…Ø­ØªÙˆØ§*

        .. code-block:: python

            from pasban.detector import WordDetector

            def validate_persian_content(content: str, threshold: float = 20.0) -> tuple[bool, str]:
                """Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ù…Ø­ØªÙˆØ§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯Ù‡Ø§ÛŒ Ù¾Ø§Ø±Ø³ÛŒ Ø³Ø±Ù‡."""
                detector = WordDetector()
                result = detector.detect(content)

                if result.foreign_percentage > threshold:
                    message = f"Ù…Ø­ØªÙˆØ§ Ø´Ø§Ù…Ù„ {result.foreign_percentage:.1f}% ÙˆØ§Ú˜Ù‡ Ø¨ÛŒÚ¯Ø§Ù†Ù‡ Ø§Ø³Øª (Ø¢Ø³ØªØ§Ù†Ù‡: {threshold}%)"
                    return False, message

                return True, "Ù…Ø­ØªÙˆØ§ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯Ù‡Ø§ÛŒ Ù¾Ø§Ø±Ø³ÛŒ Ø³Ø±Ù‡ Ø±Ø§ Ø±Ø¹Ø§ÛŒØª Ù…ÛŒâ€ŒÚ©Ù†Ø¯"

            # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± Ø³Ø§Ù…Ø§Ù†Ù‡ Ù…Ø¯ÛŒØ±ÛŒØª Ù…Ø­ØªÙˆØ§
            article_text = "Ø§ÛŒÙ† Ù…Ù‚Ø§Ù„Ù‡ Ø¯Ø±Ø¨Ø§Ø±Ù‡Ù” Ø±Ø§ÛŒØ§Ù†Ù‡ Ø§Ø³Øª..."
            is_valid, message = validate_persian_content(article_text)
            if not is_valid:
                print(f"Ø±Ø¯ Ø´Ø¯: {message}")

        *Ø®Ø· Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ†*

        .. code-block:: python

            from pasban.detector import WordDetector
            from pasban.normalizer.text_normalizer import WordNormalizer

            class PersianTextProcessor:
                def __init__(self):
                    self.detector = WordDetector()

                def process(self, text: str) -> dict:
                    """Ø®Ø· Ù„ÙˆÙ„Ù‡ Ú©Ø§Ù…Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ†."""
                    # Ú¯Ø§Ù… Û±: Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ
                    normalized = WordNormalizer.normalize_text(text)

                    # Ú¯Ø§Ù… Û²: Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ ÙˆØ§Ú˜Ú¯Ø§Ù† Ø¨ÛŒÚ¯Ø§Ù†Ù‡
                    result = self.detector.detect(normalized)

                    # Ú¯Ø§Ù… Û³: ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´
                    report = {
                        "Ù…ØªÙ†_Ø§ØµÙ„ÛŒ": text,
                        "Ù…ØªÙ†_Ù†Ø±Ù…Ø§Ù„â€ŒØ´Ø¯Ù‡": normalized,
                        "ÙˆØ§Ú˜Ú¯Ø§Ù†_Ø¨ÛŒÚ¯Ø§Ù†Ù‡": result.words,
                        "Ø¢Ù…Ø§Ø±": {
                            "Ú©Ù„_ÙˆØ§Ú˜Ú¯Ø§Ù†": result.total_words,
                            "ØªØ¹Ø¯Ø§Ø¯_Ø¨ÛŒÚ¯Ø§Ù†Ù‡": result.count,
                            "Ø¯Ø±ØµØ¯_Ø¨ÛŒÚ¯Ø§Ù†Ù‡": result.foreign_percentage
                        },
                        "Ú©ÛŒÙÛŒØª": self._assess_quality(result.foreign_percentage)
                    }

                    return report

                def _assess_quality(self, percentage: float) -> str:
                    if percentage < 5:
                        return "Ø¹Ø§Ù„ÛŒ"
                    elif percentage < 15:
                        return "Ø®ÙˆØ¨"
                    elif percentage < 30:
                        return "Ù‚Ø§Ø¨Ù„â€ŒÙ‚Ø¨ÙˆÙ„"
                    return "Ø¶Ø¹ÛŒÙ"

            # Ø§Ø³ØªÙØ§Ø¯Ù‡
            processor = PersianTextProcessor()
            report = processor.process("Ø§ÛŒÙ† Ù…ØªÙ† Ø´Ø§Ù…Ù„ Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ± Ø§Ø³Øª.")
            print(f"Ú©ÛŒÙÛŒØª: {report['Ú©ÛŒÙÛŒØª']}")
            print(f"ÙˆØ§Ú˜Ú¯Ø§Ù† Ø¨ÛŒÚ¯Ø§Ù†Ù‡: {report['ÙˆØ§Ú˜Ú¯Ø§Ù†_Ø¨ÛŒÚ¯Ø§Ù†Ù‡']}")

        *Ú©Ø§Ø±Ø¨Ø±Ø¯Ù‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ*

        .. code-block:: python

            from pasban.detector import WordDetector

            def create_learning_material(text: str) -> str:
                """Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø­ØªÙˆØ§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ø¨Ø±Ø§Ø¨Ø±Ù‡Ø§ÛŒ Ù¾Ø§Ø±Ø³ÛŒ."""
                detector = WordDetector()
                result = detector.detect(text)

                if result.count == 0:
                    return "âœ… Ø§ÛŒÙ† Ù…ØªÙ† Ø³Ø±Ø§Ø³Ø± Ù¾Ø§Ø±Ø³ÛŒ Ø§Ø³Øª!"

                output = ["ğŸ“š ÙˆØ§Ú˜Ú¯Ø§Ù† Ø¨ÛŒÚ¯Ø§Ù†Ù‡ Ùˆ Ø¨Ø±Ø§Ø¨Ø±Ù‡Ø§ÛŒ Ù¾Ø§Ø±Ø³ÛŒ Ø¢Ù†â€ŒÙ‡Ø§:\n"]

                for foreign, persian in result.words.items():
                    if foreign != persian:
                        output.append(f"âŒ {foreign} â†’ âœ… {persian}")
                    else:
                        output.append(f"âš ï¸ {foreign} (Ø¨Ø±Ø§Ø¨Ø± Ù¾Ø§Ø±Ø³ÛŒ Ù†Ø¯Ø§Ø±Ø¯)")

                output.append(f"\nğŸ“Š Ø¢Ù…Ø§Ø±: {result.count} ÙˆØ§Ú˜Ù‡ Ø¨ÛŒÚ¯Ø§Ù†Ù‡ Ø§Ø² {result.total_words} ÙˆØ§Ú˜Ù‡ ({result.foreign_percentage:.1f}%)")

                return "\n".join(output)

            # Ø§Ø³ØªÙØ§Ø¯Ù‡
            student_text = "Ù…Ù† Ø¨Ø§ Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ± Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ù… Ùˆ Ø§Ø² Ø§ÛŒÙ†ØªØ±Ù†Øª Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ù…."
            learning_material = create_learning_material(student_text)
            print(learning_material)

        *Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ Ø§Ø³Ù†Ø§Ø¯*

        .. code-block:: python

            from pasban.detector import WordDetector
            from pathlib import Path
            import json

            def process_documents(input_dir: str, output_file: str):
                """Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø§Ø³Ù†Ø§Ø¯ Ú†Ù†Ø¯Ú¯Ø§Ù†Ù‡ Ùˆ ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ Ø¬Ø§Ù…Ø¹."""
                detector = WordDetector()
                results = []

                for file_path in Path(input_dir).glob("*.txt"):
                    with open(file_path, 'r', encoding='utf-8') as f:
                        text = f.read()

                    result = detector.detect(text)

                    results.append({
                        "Ù†Ø§Ù…_Ù¾Ø±ÙˆÙ†Ø¯Ù‡": file_path.name,
                        "Ú©Ù„_ÙˆØ§Ú˜Ú¯Ø§Ù†": result.total_words,
                        "ØªØ¹Ø¯Ø§Ø¯_Ø¨ÛŒÚ¯Ø§Ù†Ù‡": result.count,
                        "Ø¯Ø±ØµØ¯_Ø¨ÛŒÚ¯Ø§Ù†Ù‡": result.foreign_percentage,
                        "ÙˆØ§Ú˜Ú¯Ø§Ù†_Ø¨ÛŒÚ¯Ø§Ù†Ù‡": list(result.words.keys())
                    })

                # Ø°Ø®ÛŒØ±Ù‡ Ú¯Ø²Ø§Ø±Ø´
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(results, f, ensure_ascii=False, indent=2)

                return results

            # Ø§Ø³ØªÙØ§Ø¯Ù‡
            results = process_documents("./documents", "./report.json")
            print(f"{len(results)} Ø³Ù†Ø¯ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯")


FAQ | Ù¾Ø±Ø³Ø´â€ŒÙ‡Ø§ÛŒ Ù…ØªØ¯Ø§ÙˆÙ„
----------------------

.. tab-set::

    .. tab-item:: English

        .. dropdown:: Why are some Arabic words not detected?
            :open:

            Pasban focuses on non-Persian words. Many Arabic words are considered part of Persian vocabulary due to historical linguistic integration. If you need to detect Arabic words specifically, you can extend the database with additional Arabic terms.

        .. dropdown:: Can I use Pasban offline?

            Yes! After the initial database download, Pasban works completely offline. The database is cached locally and doesn't require internet connection for subsequent uses.

        .. dropdown:: How accurate is the detection?

            Accuracy depends on the engine and settings:

            * WordDetector: ~98% accuracy with default settings
            * WordDetectorRegex: ~99% accuracy
            * Both achieve best results with normalization and contextual cleaning enabled

        .. dropdown:: Can I add custom words to the database?

            Yes! Use ``WordRepo`` to add, update, or remove words:

            .. code-block:: python

                from pasban.db import WordRepo

                repo = WordRepo()
                repo.add_word("Ú©Ø§Ø³ØªÙˆÙ…", "Ø³ÙØ§Ø±Ø´ÛŒ")

                # Reload detector after updates
                detector.reload()

        .. dropdown:: What's the difference between count and unique_count?

            * ``count``: Total number of foreign word occurrences (includes duplicates)
            * ``unique_count``: Number of unique foreign words

            Example: "Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ± Ùˆ Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±" â†’ count=2, unique_count=1

        .. dropdown:: How do I handle very large texts?

            For large texts (>10MB):

            1. Split text into chunks
            2. Process each chunk separately
            3. Aggregate results
            4. Use ``WordDetector`` (much faster for large texts)

        .. dropdown:: Can I use Pasban in web applications?

            Yes! Pasban is thread-safe and can be used in web applications. Create a single detector instance and reuse it across requests for best performance.

        .. dropdown:: How often is the database updated?

            The database is community-maintained and updated regularly. You can contribute new words or corrections through the project repository.

    .. tab-item:: Ù¾Ø§Ø±Ø³ÛŒ

        .. dropdown:: Ú†Ø±Ø§ Ø¨Ø±Ø®ÛŒ ÙˆØ§Ú˜Ú¯Ø§Ù† Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù†Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯ØŸ
            :open:

            Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ Ù¾Ø§Ø³Ø¨Ø§Ù† Ø¯Ø± Ø§Ù…Ø§ÛŒÙ‡ Ú¯Ø³ØªØ±Ø´ Ø§Ø³Øª Ùˆ Ø´Ø§ÛŒØ¯ Ø¨Ø±Ø®ÛŒ ÙˆØ§Ú˜Ú¯Ø§Ù† Ø§ÛŒÙ†Ú© Ø¯Ø± Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ Ù†Ø¨Ø§Ø´Ù†Ø¯.

        .. dropdown:: Ø¢ÛŒØ§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù… Ø§Ø² Ù¾Ø§Ø³Ø¨Ø§Ù† Ø¨Ù‡â€ŒØµÙˆØ±Øª Ø¢ÙÙ„Ø§ÛŒÙ† Ø¨Ú©Ø§Ø± Ú¯ÛŒØ±Ù…ØŸ

            Ø¨Ù„Ù‡! Ù¾Ø³ Ø§Ø² Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ Ù†Ø®Ø³ØªÛŒÙ† Ù¾Ø§ÛŒÚ¯Ø§Ù‡â€ŒØ¯Ø§Ø¯Ù‡ØŒ Ù¾Ø§Ø³Ø¨Ø§Ù† Ø¨Ù‡â€ŒØ·ÙˆØ± Ú©Ø§Ù…Ù„ Ø¢ÙÙ„Ø§ÛŒÙ† Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ù¾Ø§ÛŒÚ¯Ø§Ù‡â€ŒØ¯Ø§Ø¯Ù‡ Ø¨Ù‡â€ŒØµÙˆØ±Øª Ù…Ø­Ù„ÛŒ Ø­Ø§ÙØ¸Ù‡â€ŒÙ†Ù‡Ø§Ù†ÛŒ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ùˆ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø¹Ø¯ÛŒ Ù†ÛŒØ§Ø²ÛŒ Ø¨Ù‡ Ù¾ÛŒÙˆÙ†Ø¯ Ø§ÛŒÙ†ØªØ±Ù†Øª Ù†Ø¯Ø§Ø±Ø¯.

        .. dropdown:: Ù…ÙˆØ´Ú©Ø§ÙÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ú†Ù‡ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ø§Ø³ØªØŸ

            Ù…ÙˆØ´Ú©Ø§ÙÛŒ Ø¨Ù‡ Ù…ÙˆØªÙˆØ± Ùˆ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¨Ø³ØªÚ¯ÛŒ Ø¯Ø§Ø±Ø¯:

            * WordDetector: Ù…ÙˆØ´Ú©Ø§ÙÛŒ ~Û¹Û¸Ùª Ø¨Ø§ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù¾ÛŒØ´â€ŒÙØ±Ø¶
            * WordDetectorRegex: Ù…ÙˆØ´Ú©Ø§ÙÛŒ ~Û¹Û¹Ùª
            * Ù‡Ø± Ø¯Ùˆ Ø¨Ø§ ÙØ¹Ø§Ù„ Ø¨ÙˆØ¯Ù† Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ùˆ Ù¾Ø§Ù„Ø§ÛŒØ´ Ø²Ù…ÛŒÙ†Ù‡â€ŒØ§ÛŒ Ø¨Ù‡ØªØ±ÛŒÙ† Ù†ØªØ§ÛŒØ¬ Ø±Ø§ Ø¯Ø§Ø±Ù†Ø¯

        .. dropdown:: Ø¢ÛŒØ§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù… ÙˆØ§Ú˜Ú¯Ø§Ù† Ø³ÙØ§Ø±Ø´ÛŒ Ø¨Ù‡ Ù¾Ø§ÛŒÚ¯Ø§Ù‡â€ŒØ¯Ø§Ø¯Ù‡ Ø¨ÛŒÙØ²Ø§ÛŒÙ…ØŸ

            Ø¨Ù„Ù‡! Ø§Ø² ``WordRepo`` Ø¨Ø±Ø§ÛŒ Ø§ÙØ²ÙˆØ¯Ù†ØŒ ÙˆÛŒØ±Ø§ÛŒØ´ ÛŒØ§ Ø­Ø°Ù ÙˆØ§Ú˜Ú¯Ø§Ù† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯:

            .. code-block:: python

                from pasban.db import WordRepo

                repo = WordRepo()
                repo.add_word("Ú©Ø§Ø³ØªÙˆÙ…", "Ø³ÙØ§Ø±Ø´ÛŒ")

                # Ø´Ù†Ø§Ø³Ø§Ú¯Ø± Ø±Ø§ Ù¾Ø³ Ø§Ø² Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ú©Ù†ÛŒØ¯
                detector.reload()

        .. dropdown:: ØªÙØ§ÙˆØª count Ùˆ unique_count Ú†ÛŒØ³ØªØŸ

            * ``count``: ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ø±Ø®Ø¯Ø§Ø¯Ù‡Ø§ÛŒ ÙˆØ§Ú˜Ù‡ Ø¨ÛŒÚ¯Ø§Ù†Ù‡ (Ø´Ø§Ù…Ù„ ØªÚ©Ø±Ø§Ø±ÛŒâ€ŒÙ‡Ø§)
            * ``unique_count``: ØªØ¹Ø¯Ø§Ø¯ ÙˆØ§Ú˜Ú¯Ø§Ù† Ø¨ÛŒÚ¯Ø§Ù†Ù‡ ÛŒÚ©ØªØ§

            Ù…Ø«Ø§Ù„: "Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ± Ùˆ Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±" â† count=2ØŒ unique_count=1

        .. dropdown:: Ú†Ú¯ÙˆÙ†Ù‡ Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ Ø¨Ø³ÛŒØ§Ø± Ø¨Ø²Ø±Ú¯ Ø±Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ú©Ù†Ù…ØŸ

            Ø¨Ø±Ø§ÛŒ Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ Ø¨Ø²Ø±Ú¯ (Ø¨ÛŒØ´ Ø§Ø² Û±Û° Ù…Ú¯Ø§Ø¨Ø§ÛŒØª):

            1. Ù…ØªÙ† Ø±Ø§ Ø¨Ù‡ Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ Ú©ÙˆÚ†Ú©â€ŒØªØ± ØªÙ‚Ø³ÛŒÙ… Ú©Ù†ÛŒØ¯
            2. Ù‡Ø± Ø¨Ø®Ø´ Ø±Ø§ Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ú©Ù†ÛŒØ¯
            3. Ù†ØªØ§ÛŒØ¬ Ø±Ø§ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ú©Ù†ÛŒØ¯
            4. Ø§Ø² ``WordDetector`` Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯ (Ø¨Ø±Ø§ÛŒ Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ Ø¨Ø²Ø±Ú¯ Ø¨Ø³ÛŒØ§Ø± Ø³Ø±ÛŒØ¹â€ŒØªØ± Ø§Ø³Øª)

        .. dropdown:: Ø¢ÛŒØ§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù… Ø§Ø² Ù¾Ø§Ø³Ø¨Ø§Ù† Ø¯Ø± Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ‡Ø§ÛŒ ÙˆØ¨ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†Ù…ØŸ

            Ø¨Ù„Ù‡! Ù¾Ø§Ø³Ø¨Ø§Ù† Ø§ÛŒÙ…Ù† Ø¯Ø± Ø¨Ø±Ø§Ø¨Ø± Ù†Ø® (thread-safe) Ø§Ø³Øª Ùˆ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø¯Ø± Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ‡Ø§ÛŒ ÙˆØ¨ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´ÙˆØ¯. ÛŒÚ© Ù†Ù…ÙˆÙ†Ù‡ Ø´Ù†Ø§Ø³Ø§Ú¯Ø± Ø¨Ø³Ø§Ø²ÛŒØ¯ Ùˆ Ø¢Ù† Ø±Ø§ Ø¯Ø± Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§ Ù…Ø¬Ø¯Ø¯Ø§Ù‹ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯ ØªØ§ Ú©Ø§Ø±Ø§ÛŒÛŒ Ø¨Ù‡ÛŒÙ†Ù‡ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒØ¯.

        .. dropdown:: Ù¾Ø§ÛŒÚ¯Ø§Ù‡â€ŒØ¯Ø§Ø¯Ù‡ Ú†Ù‡ Ø§Ù†Ø¯Ø§Ø²Ù‡ ÛŒÚ©â€ŒØ¨Ø§Ø± Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù…ÛŒâ€ŒØ´ÙˆØ¯ØŸ

            Ù¾Ø§ÛŒÚ¯Ø§Ù‡â€ŒØ¯Ø§Ø¯Ù‡ ØªÙˆØ³Ø· Ø¬Ø§Ù…Ø¹Ù‡ Ù†Ú¯Ù‡Ø¯Ø§Ø±ÛŒ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ùˆ Ø¨Ù‡â€ŒØ·ÙˆØ± Ù…Ù†Ø¸Ù… Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø¯. Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ ÙˆØ§Ú˜Ú¯Ø§Ù† Ø¬Ø¯ÛŒØ¯ ÛŒØ§ Ø§ØµÙ„Ø§Ø­Ø§Øª Ø±Ø§ Ø§Ø² Ø·Ø±ÛŒÙ‚ Ù…Ø®Ø²Ù† Ù¾Ø±ÙˆÚ˜Ù‡ Ù…Ø´Ø§Ø±Ú©Øª Ú©Ù†ÛŒØ¯.


Troubleshooting | Ø¹ÛŒØ¨â€ŒÛŒØ§Ø¨ÛŒ
---------------------------

.. tab-set::

    .. tab-item:: English

        *Common Issues and Solutions*

        .. grid:: 1
            :gutter: 3

            .. grid-item-card:: Issue: Slow Performance
                :class-card: sd-border-warning

                **Symptoms:** Detection takes too long

                **Solutions:**

                * Use ``WordDetector`` instead of ``WordDetectorRegex``
                * Reuse detector instances instead of re-initializing
                * Consider disabling contextual cleaning for simple texts
                * Process large texts in smaller chunks

                .. code-block:: python

                    # âœ… Good
                    detector = WordDetector()  # Initialize once
                    for text in texts:
                        result = detector.detect(text)

                    # âŒ Slow
                    for text in texts:
                        detector = WordDetector()  # Re-initializing!
                        result = detector.detect(text)

            .. grid-item-card:: Issue: Incorrect Detection
                :class-card: sd-border-danger

                **Symptoms:** Words not detected or false positives

                **Solutions:**

                * Enable normalization: ``detect(text, normalize=True)``
                * Enable contextual cleaning: ``detect(text, contextual=True)``
                * Try ``WordDetectorRegex`` for better accuracy
                * Check if word exists in database: ``repo.get_persian("word")``
                * Add missing words to database: ``repo.add_word("word", "persian")``

                .. code-block:: python

                    from pasban.db import WordRepo

                    repo = WordRepo()

                    # Check if word exists
                    persian = repo.get_persian("Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±")
                    if not persian:
                        # Add it
                        repo.add_word("Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±", "Ø±Ø§ÛŒØ§Ù†Ù‡")

            .. grid-item-card:: Issue: Database Not Loading
                :class-card: sd-border-danger

                **Symptoms:** Import errors or missing database

                **Solutions:**

                * Ensure internet connection for initial download
                * Check installation: ``pip install --upgrade pasban``
                * Verify database path permissions
                * Try manual reload: ``repo.get_all_words(reload=True)``

                .. code-block:: python

                    from pasban.db import WordRepo

                    try:
                        repo = WordRepo()
                        words = repo.get_all_words(reload=True)
                        print(f"Loaded {len(words)} words")
                    except Exception as e:
                        print(f"Error: {e}")

            .. grid-item-card:: Issue: Memory Usage
                :class-card: sd-border-warning

                **Symptoms:** High memory consumption

                **Solutions:**

                * Process large texts in chunks
                * Don't store all DetectData objects in memory
                * Extract only needed information from results
                * Use ``detect_words()`` instead of ``detect()`` if you only need the word mappings

                .. code-block:: python

                    # âœ… Memory efficient
                    detector = WordDetector()
                    for text in large_text_list:
                        words = detector.detect_words(text)  # Only words dict
                        process(words)
                        # words is garbage collected after loop

                    # âŒ Memory intensive
                    results = []
                    for text in large_text_list:
                        results.append(detector.detect(text))  # Stores everything

            .. grid-item-card:: Issue: Encoding Problems
                :class-card: sd-border-info

                **Symptoms:** Garbled text or incorrect characters

                **Solutions:**

                * Always use UTF-8 encoding for files
                * Normalize text before processing
                * Check input source encoding

                .. code-block:: python

                    # âœ… Correct encoding
                    with open('file.txt', 'r', encoding='utf-8') as f:
                        text = f.read()

                    # Normalize to ensure consistent encoding
                    from pasban.normalizer.text_normalizer import WordNormalizer
                    text = WordNormalizer.normalize_text(text)

    .. tab-item:: Ù¾Ø§Ø±Ø³ÛŒ

        *Ù…Ø³Ø§Ø¦Ù„ Ø±Ø§ÛŒØ¬ Ùˆ Ø±Ø§Ù‡â€ŒØ­Ù„â€ŒÙ‡Ø§*

        .. grid:: 1
            :gutter: 3

            .. grid-item-card:: Ù…Ø³Ø¦Ù„Ù‡: Ú©Ø§Ø±Ø§ÛŒÛŒ Ù¾Ø§ÛŒÛŒÙ†
                :class-card: sd-border-warning

                **Ø¹Ù„Ø§Ø¦Ù…:** Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø²Ù…Ø§Ù† Ø²ÛŒØ§Ø¯ÛŒ Ù…ÛŒâ€ŒØ¨Ø±Ø¯

                **Ø±Ø§Ù‡â€ŒØ­Ù„â€ŒÙ‡Ø§:**

                * Ø§Ø² ``WordDetector`` Ø¨Ù‡â€ŒØ¬Ø§ÛŒ ``WordDetectorRegex`` Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯
                * Ø§Ø² Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø´Ù†Ø§Ø³Ø§Ú¯Ø± Ù…Ø¬Ø¯Ø¯Ø§Ù‹ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯ Ø¨Ù‡â€ŒØ¬Ø§ÛŒ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø¯ÙˆØ¨Ø§Ø±Ù‡
                * Ù¾Ø§Ù„Ø§ÛŒØ´ Ø²Ù…ÛŒÙ†Ù‡â€ŒØ§ÛŒ Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ Ø³Ø§Ø¯Ù‡ ØºÛŒØ±ÙØ¹Ø§Ù„ Ú©Ù†ÛŒØ¯
                * Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ Ø¨Ø²Ø±Ú¯ Ø±Ø§ Ø¨Ù‡ Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ Ú©ÙˆÚ†Ú©â€ŒØªØ± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ú©Ù†ÛŒØ¯

                .. code-block:: python

                    # âœ… Ø®ÙˆØ¨
                    detector = WordDetector()  # ÛŒÚ©â€ŒØ¨Ø§Ø± Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ
                    for text in texts:
                        result = detector.detect(text)

                    # âŒ Ú©Ù†Ø¯
                    for text in texts:
                        detector = WordDetector()  # Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø¯ÙˆØ¨Ø§Ø±Ù‡!
                        result = detector.detect(text)

            .. grid-item-card:: Ù…Ø³Ø¦Ù„Ù‡: Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù†Ø§Ø¯Ø±Ø³Øª
                :class-card: sd-border-danger

                **Ø¹Ù„Ø§Ø¦Ù…:** ÙˆØ§Ú˜Ú¯Ø§Ù† Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù†Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯ ÛŒØ§ Ù…Ø«Ø¨Øª Ú©Ø§Ø°Ø¨ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯

                **Ø±Ø§Ù‡â€ŒØ­Ù„â€ŒÙ‡Ø§:**

                * Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø±Ø§ ÙØ¹Ø§Ù„ Ú©Ù†ÛŒØ¯: ``detect(text, normalize=True)``
                * Ù¾Ø§Ù„Ø§ÛŒØ´ Ø²Ù…ÛŒÙ†Ù‡â€ŒØ§ÛŒ Ø±Ø§ ÙØ¹Ø§Ù„ Ú©Ù†ÛŒØ¯: ``detect(text, contextual=True)``
                * ``WordDetectorRegex`` Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ù…ÙˆØ´Ú©Ø§ÙÛŒ Ø¨Ù‡ØªØ± Ø§Ù…ØªØ­Ø§Ù† Ú©Ù†ÛŒØ¯
                * Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯ ÙˆØ§Ú˜Ù‡ Ø¯Ø± Ù¾Ø§ÛŒÚ¯Ø§Ù‡â€ŒØ¯Ø§Ø¯Ù‡ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯: ``repo.get_persian("word")``
                * ÙˆØ§Ú˜Ú¯Ø§Ù† Ú¯Ù…â€ŒØ´Ø¯Ù‡ Ø±Ø§ Ø¨Ù‡ Ù¾Ø§ÛŒÚ¯Ø§Ù‡â€ŒØ¯Ø§Ø¯Ù‡ Ø¨ÛŒÙØ²Ø§ÛŒÛŒØ¯: ``repo.add_word("word", "persian")``

                .. code-block:: python

                    from pasban.db import WordRepo

                    repo = WordRepo()

                    # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ ÙˆØ§Ú˜Ù‡
                    persian = repo.get_persian("Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±")
                    if not persian:
                        # Ø§ÙØ²ÙˆØ¯Ù† Ø¢Ù†
                        repo.add_word("Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±", "Ø±Ø§ÛŒØ§Ù†Ù‡")

            .. grid-item-card:: Ù…Ø³Ø¦Ù„Ù‡: Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù†Ø´Ø¯Ù† Ù¾Ø§ÛŒÚ¯Ø§Ù‡â€ŒØ¯Ø§Ø¯Ù‡
                :class-card: sd-border-danger

                **Ø¹Ù„Ø§Ø¦Ù…:** Ø®Ø·Ø§Ù‡Ø§ÛŒ ÙˆØ§Ø±Ø¯ Ú©Ø±Ø¯Ù† ÛŒØ§ Ù¾Ø§ÛŒÚ¯Ø§Ù‡â€ŒØ¯Ø§Ø¯Ù‡ Ú¯Ù…â€ŒØ´Ø¯Ù‡

                **Ø±Ø§Ù‡â€ŒØ­Ù„â€ŒÙ‡Ø§:**

                * Ø§Ø² Ù¾ÛŒÙˆÙ†Ø¯ Ø§ÛŒÙ†ØªØ±Ù†Øª Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ Ù†Ø®Ø³ØªÛŒÙ† Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø­Ø§ØµÙ„ Ú©Ù†ÛŒØ¯
                * Ù†ØµØ¨ Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯: ``pip install --upgrade pasban``
                * Ù…Ø¬ÙˆØ²Ù‡Ø§ÛŒ Ù…Ø³ÛŒØ± Ù¾Ø§ÛŒÚ¯Ø§Ù‡â€ŒØ¯Ø§Ø¯Ù‡ Ø±Ø§ ØªØ£ÛŒÛŒØ¯ Ú©Ù†ÛŒØ¯
                * Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø³ØªÛŒ Ø±Ø§ Ø§Ù…ØªØ­Ø§Ù† Ú©Ù†ÛŒØ¯: ``repo.get_all_words(reload=True)``

                .. code-block:: python

                    from pasban.db import WordRepo

                    try:
                        repo = WordRepo()
                        words = repo.get_all_words(reload=True)
                        print(f"{len(words)} ÙˆØ§Ú˜Ù‡ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯")
                    except Exception as e:
                        print(f"Ø®Ø·Ø§: {e}")

            .. grid-item-card:: Ù…Ø³Ø¦Ù„Ù‡: Ù…ØµØ±Ù Ø­Ø§ÙØ¸Ù‡
                :class-card: sd-border-warning

                **Ø¹Ù„Ø§Ø¦Ù…:** Ù…ØµØ±Ù Ø¨Ø§Ù„Ø§ÛŒ Ø­Ø§ÙØ¸Ù‡

                **Ø±Ø§Ù‡â€ŒØ­Ù„â€ŒÙ‡Ø§:**

                * Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ Ø¨Ø²Ø±Ú¯ Ø±Ø§ Ø¨Ù‡ Ø¨Ø®Ø´â€ŒÙ‡Ø§ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ú©Ù†ÛŒØ¯
                * Ù‡Ù…Ù‡Ù” Ø§Ø´ÛŒØ§Ø¡ DetectData Ø±Ø§ Ø¯Ø± Ø­Ø§ÙØ¸Ù‡ Ø°Ø®ÛŒØ±Ù‡ Ù†Ú©Ù†ÛŒØ¯
                * ÙÙ‚Ø· Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² Ø±Ø§ Ø§Ø² Ù†ØªØ§ÛŒØ¬ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù†ÛŒØ¯
                * Ø§Ú¯Ø± ÙÙ‚Ø· Ù†Ú¯Ø§Ø´Øª ÙˆØ§Ú˜Ú¯Ø§Ù† Ø±Ø§ Ù†ÛŒØ§Ø² Ø¯Ø§Ø±ÛŒØ¯ØŒ Ø§Ø² ``detect_words()`` Ø¨Ù‡â€ŒØ¬Ø§ÛŒ ``detect()`` Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯

                .. code-block:: python

                    # âœ… Ú©Ø§Ø±Ø¢Ù…Ø¯ Ø¯Ø± Ø­Ø§ÙØ¸Ù‡
                    detector = WordDetector()
                    for text in large_text_list:
                        words = detector.detect_words(text)  # ÙÙ‚Ø· Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ ÙˆØ§Ú˜Ú¯Ø§Ù†
                        process(words)
                        # words Ù¾Ø³ Ø§Ø² Ø­Ù„Ù‚Ù‡ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø²Ø¨Ø§Ù„Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯

                    # âŒ Ù¾Ø±Ù…ØµØ±Ù Ø­Ø§ÙØ¸Ù‡
                    results = []
                    for text in large_text_list:
                        results.append(detector.detect(text))  # Ù‡Ù…Ù‡ Ú†ÛŒØ² Ø±Ø§ Ø°Ø®ÛŒØ±Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯

            .. grid-item-card:: Ù…Ø³Ø¦Ù„Ù‡: Ù…Ø´Ú©Ù„Ø§Øª Ú©Ø¯Ú¯Ø°Ø§Ø±ÛŒ
                :class-card: sd-border-info

                **Ø¹Ù„Ø§Ø¦Ù…:** Ù…ØªÙ† Ù…Ø®Ø¯ÙˆØ´ ÛŒØ§ Ù†ÙˆÛŒØ³Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ø§Ø¯Ø±Ø³Øª

                **Ø±Ø§Ù‡â€ŒØ­Ù„â€ŒÙ‡Ø§:**

                * Ù‡Ù…ÙˆØ§Ø±Ù‡ Ø§Ø² Ú©Ø¯Ú¯Ø°Ø§Ø±ÛŒ UTF-8 Ø¨Ø±Ø§ÛŒ Ù¾Ø±ÙˆÙ†Ø¯Ù‡â€ŒÙ‡Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯
                * Ù…ØªÙ† Ø±Ø§ Ù¾ÛŒØ´ Ø§Ø² Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù†Ø±Ù…Ø§Ù„ Ú©Ù†ÛŒØ¯
                * Ú©Ø¯Ú¯Ø°Ø§Ø±ÛŒ Ù…Ù†Ø¨Ø¹ ÙˆØ±ÙˆØ¯ÛŒ Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯

                .. code-block:: python

                    # âœ… Ú©Ø¯Ú¯Ø°Ø§Ø±ÛŒ ØµØ­ÛŒØ­
                    with open('file.txt', 'r', encoding='utf-8') as f:
                        text = f.read()

                    # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ú©Ø¯Ú¯Ø°Ø§Ø±ÛŒ ÛŒÚ©Ø³Ø§Ù†
                    from pasban.normalizer.text_normalizer import WordNormalizer
                    text = WordNormalizer.normalize_text(text)


API Reference Summary | Ø®Ù„Ø§ØµÙ‡ Ù…Ø±Ø¬Ø¹ API
---------------------------------------

.. tab-set::

    .. tab-item:: English

        *Quick Reference*

        **Detection Engines**

        .. code-block:: python

            from pasban.detector import WordDetector, WordDetectorRegex

            # Fast detection (recommended)
            detector = WordDetector()
            result = detector.detect(text, normalize=True, contextual=True)
            words = detector.detect_words(text)
            detector.reload()

            # Accurate detection
            detector_regex = WordDetectorRegex()
            result = detector_regex.detect(text, normalize=True, contextual=True)
            words_list = detector_regex.find_words_in_text(text)

        **Word Repository**

        .. code-block:: python

            from pasban.db import WordRepo

            repo = WordRepo()
            all_words = repo.get_all_words(reload=False)
            results = repo.search_word(search_term, limit=5)
            repo.add_word(foreign, persian)
            repo.update_word(foreign, persian)
            repo.remove_word(foreign)
            persian = repo.get_persian(foreign)

        **Normalization**

        .. code-block:: python

            from pasban.normalizer.text_normalizer import WordNormalizer

            normalized = WordNormalizer.normalize_text(text)

        **Contextual Cleaning**

        .. code-block:: python

            from pasban.normalizer.contextual_remover import contextual_cleaner

            cleaned = contextual_cleaner.clean_text(text)

        **DetectData Attributes**

        .. code-block:: python

            result = detector.detect(text)

            result.foreign_words      # list[str]
            result.words              # dict[str, str]
            result.text               # str
            result.count              # int
            result.unique_count       # int
            result.total_words        # int
            result.foreign_percentage # float
            result.to_text            # str
            result.to_summary_text    # str

    .. tab-item:: Ù¾Ø§Ø±Ø³ÛŒ

        *Ù…Ø±Ø¬Ø¹ Ø³Ø±ÛŒØ¹*

        **Ù…ÙˆØªÙˆØ±Ù‡Ø§ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ**

        .. code-block:: python

            from pasban.detector import WordDetector, WordDetectorRegex

            # Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø³Ø±ÛŒØ¹ (Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ù…ÛŒâ€ŒØ´ÙˆØ¯)
            detector = WordDetector()
            result = detector.detect(text, normalize=True, contextual=True)
            words = detector.detect_words(text)
            detector.reload()

            # Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø¯Ù‚ÛŒÙ‚
            detector_regex = WordDetectorRegex()
            result = detector_regex.detect(text, normalize=True, contextual=True)
            words_list = detector_regex.find_words_in_text(text)

        **Ù…Ø®Ø²Ù† ÙˆØ§Ú˜Ú¯Ø§Ù†**

        .. code-block:: python

            from pasban.db import WordRepo

            repo = WordRepo()
            all_words = repo.get_all_words(reload=False)
            results = repo.search_word(search_term, limit=5)
            repo.add_word(foreign, persian)
            repo.update_word(foreign, persian)
            repo.remove_word(foreign)
            persian = repo.get_persian(foreign)

        **Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ**

        .. code-block:: python

            from pasban.normalizer.text_normalizer import WordNormalizer

            normalized = WordNormalizer.normalize_text(text)

        **Ù¾Ø§Ù„Ø§ÛŒØ´ Ø²Ù…ÛŒÙ†Ù‡â€ŒØ§ÛŒ**

        .. code-block:: python

            from pasban.normalizer.contextual_remover import contextual_cleaner

            cleaned = contextual_cleaner.clean_text(text)

        **ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ DetectData**

        .. code-block:: python

            result = detector.detect(text)

            result.foreign_words      # list[str]
            result.words              # dict[str, str]
            result.text               # str
            result.count              # int
            result.unique_count       # int
            result.total_words        # int
            result.foreign_percentage # float
            result.to_text            # str
            result.to_summary_text    # str


Contributing | Ù…Ø´Ø§Ø±Ú©Øª
---------------------

.. tab-set::

    .. tab-item:: English

        We welcome contributions to Pasban! Here's how you can help:

        **Ways to Contribute**

        * Report bugs or incorrect detections
        * Improve documentation
        * Submit performance optimizations
        * Add new features
        * Write tests

        **Reporting Issues**

        Please include:

        * Clear description of the issue
        * Example text demonstrating the problem
        * Expected vs actual behavior
        * Python version and Pasban version
        * Full error traceback (if applicable)

        **Code Style**

        * Follow PEP 8 guidelines
        * Write docstrings for all public functions
        * Include type hints
        * Add tests for new features
        * Update documentation

    .. tab-item:: Ù¾Ø§Ø±Ø³ÛŒ

        Ø§Ø² Ù…Ø´Ø§Ø±Ú©Øª Ø´Ù…Ø§ Ø¯Ø± Ù¾Ø§Ø³Ø¨Ø§Ù† Ø§Ø³ØªÙ‚Ø¨Ø§Ù„ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…! Ø±Ø§Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø´Ø§Ø±Ú©Øª:

        **Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ù…Ø´Ø§Ø±Ú©Øª**

        * Ú¯Ø²Ø§Ø±Ø´ Ø§Ø´Ú©Ø§Ù„Ø§Øª ÛŒØ§ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒâ€ŒÙ‡Ø§ÛŒ Ù†Ø§Ø¯Ø±Ø³Øª
        * Ø¨Ù‡Ø¨ÙˆØ¯ Ù…Ø³ØªÙ†Ø¯Ø§Øª
        * Ø§Ø±Ø³Ø§Ù„ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒâ€ŒÙ‡Ø§ÛŒ Ú©Ø§Ø±Ø§ÛŒÛŒ
        * Ø§ÙØ²ÙˆØ¯Ù† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯
        * Ù†ÙˆØ´ØªÙ† Ø¢Ø²Ù…ÙˆÙ†â€ŒÙ‡Ø§


        **Ú¯Ø²Ø§Ø±Ø´ Ù…Ø³Ø§Ø¦Ù„**

        Ù„Ø·ÙØ§Ù‹ Ø´Ø§Ù…Ù„ Ø¨Ø§Ø´Ø¯:

        * ØªÙˆØ¶ÛŒØ­ ÙˆØ§Ø¶Ø­ Ù…Ø³Ø¦Ù„Ù‡
        * Ù…ØªÙ† Ù†Ù…ÙˆÙ†Ù‡ Ù†Ù…Ø§ÛŒØ´â€ŒØ¯Ù‡Ù†Ø¯Ù‡Ù” Ù…Ø´Ú©Ù„
        * Ø±ÙØªØ§Ø± Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø± Ø¯Ø± Ù…Ù‚Ø§Ø¨Ù„ Ø±ÙØªØ§Ø± ÙˆØ§Ù‚Ø¹ÛŒ
        * Ù†Ø³Ø®Ù‡ Ù¾Ø§ÛŒØªÙˆÙ† Ùˆ Ù†Ø³Ø®Ù‡ Ù¾Ø§Ø³Ø¨Ø§Ù†
        * Ø±Ø¯ÛŒØ§Ø¨ÛŒ Ú©Ø§Ù…Ù„ Ø®Ø·Ø§ (Ø¯Ø± ØµÙˆØ±Øª ÙˆØ¬ÙˆØ¯)

        **Ø³Ø¨Ú© Ú©Ø¯**

        * Ø±Ø§Ù‡Ù†Ù…Ø§Ù‡Ø§ÛŒ PEP 8 Ø±Ø§ Ø¯Ù†Ø¨Ø§Ù„ Ú©Ù†ÛŒØ¯
        * Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ù‡Ù” ØªÙˆØ§Ø¨Ø¹ Ø¹Ù…ÙˆÙ…ÛŒ docstring Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯
        * Ø±Ø§Ù‡Ù†Ù…Ø§Ù‡Ø§ÛŒ Ù†ÙˆØ¹ Ø±Ø§ Ø¨ÛŒÙØ²Ø§ÛŒÛŒØ¯
        * Ø¨Ø±Ø§ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ Ø¢Ø²Ù…ÙˆÙ† Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯
        * Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø±Ø§ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ú©Ù†ÛŒØ¯


License and Credits | Ù…Ø¬ÙˆØ² Ùˆ Ø§Ø¹ØªØ¨Ø§Ø±Ø§Øª
--------------------------------------

.. tab-set::

    .. tab-item:: English

        **License**

        Pasban is released under the MIT License.

        **Credits**

        * Developed and maintained by the Persian NLP community
        * Word database curated by contributors
        * Built with Python and love for Persian language

        **Acknowledgments**

        Special thanks to all contributors who help maintain and improve the word database.

    .. tab-item:: Ù¾Ø§Ø±Ø³ÛŒ

        **Ù…Ø¬ÙˆØ²**

        Ù¾Ø§Ø³Ø¨Ø§Ù† ØªØ­Øª Ù…Ø¬ÙˆØ² MIT Ù…Ù†ØªØ´Ø± Ø´Ø¯Ù‡ Ø§Ø³Øª.

        **Ø§Ø¹ØªØ¨Ø§Ø±Ø§Øª**

        * ØªÙˆØ³Ø¹Ù‡ Ùˆ Ù†Ú¯Ù‡Ø¯Ø§Ø±ÛŒ ØªÙˆØ³Ø· Ø§Ù†Ø¬Ù…Ù† Ù¾Ø§Ø³Ø¨Ø§Ù† Ø³Ø±Ù‡ Ú¯Ø±Ø§ÛŒÛŒ Ø§ÛŒØ±Ø§Ù†
        * Ù¾Ø§ÛŒÚ¯Ø§Ù‡ ÙˆØ§Ú˜Ú¯Ø§Ù† Ú¯Ø±Ø¯Ø¢ÙˆØ±ÛŒâ€ŒØ´Ø¯Ù‡ ØªÙˆØ³Ø· Ù…Ø´Ø§Ø±Ú©Øªâ€ŒÚ©Ù†Ù†Ø¯Ú¯Ø§Ù†
        * Ø³Ø§Ø®ØªÙ‡â€ŒØ´Ø¯Ù‡ Ø¨Ø§ Ù¾Ø§ÛŒØªÙˆÙ† Ùˆ Ø¹Ø´Ù‚ Ø¨Ù‡ Ø²Ø¨Ø§Ù† Ù¾Ø§Ø±Ø³ÛŒ

        **Ù‚Ø¯Ø±Ø¯Ø§Ù†ÛŒ**

        Ø³Ù¾Ø§Ø³ ÙˆÛŒÚ˜Ù‡ Ø§Ø² Ù‡Ù…Ù‡Ù” Ù…Ø´Ø§Ø±Ú©Øªâ€ŒÚ©Ù†Ù†Ø¯Ú¯Ø§Ù†ÛŒ Ú©Ù‡ Ø¨Ù‡ Ù†Ú¯Ù‡Ø¯Ø§Ø±ÛŒ Ùˆ Ø¨Ù‡Ø¨ÙˆØ¯ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ ÙˆØ§Ú˜Ú¯Ø§Ù† Ú©Ù…Ú© Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯.
